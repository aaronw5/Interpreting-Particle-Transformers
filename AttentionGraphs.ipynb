{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ebiu99d7kFz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import awkward as ak\n",
        "import uproot\n",
        "import vector\n",
        "vector.register_awkward()\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "import tarfile\n",
        "import urllib\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "#from weaver.nn.model.ParticleTransformer import ParticleTransformer\n",
        "#from weaver.utils.logger import _logger\n",
        "import torch.optim as optim\n",
        "#from EfficientParticleTransformer import EfficientParticleTransformer\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.collections import LineCollection\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OarlU9i4ea09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBYWPUDYzbzi"
      },
      "outputs": [],
      "source": [
        "!pip install mplhep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trYIGwwxzsHk"
      },
      "outputs": [],
      "source": [
        "import mplhep as hep\n",
        "\n",
        "hep.style.use(hep.style.ROOT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oxx4t8APTPZp"
      },
      "outputs": [],
      "source": [
        "def build_features_and_labels(tree, transform_features=True):\n",
        "\n",
        "    # load arrays from the tree\n",
        "    a = tree.arrays(filter_name=['part_*', 'jet_pt', 'jet_energy', 'label_*'])\n",
        "\n",
        "    # compute new features\n",
        "    a['part_mask'] = ak.ones_like(a['part_energy'])\n",
        "    a['part_pt'] = np.hypot(a['part_px'], a['part_py'])\n",
        "    a['part_pt_log'] = np.log(a['part_pt'])\n",
        "    a['part_e_log'] = np.log(a['part_energy'])\n",
        "    a['part_logptrel'] = np.log(a['part_pt']/a['jet_pt'])\n",
        "    a['part_logerel'] = np.log(a['part_energy']/a['jet_energy'])\n",
        "    a['part_deltaR'] = np.hypot(a['part_deta'], a['part_dphi'])\n",
        "    a['part_d0'] = np.tanh(a['part_d0val'])\n",
        "    a['part_dz'] = np.tanh(a['part_dzval'])\n",
        "\n",
        "    # apply standardization\n",
        "    if transform_features:\n",
        "        a['part_pt_log'] = (a['part_pt_log'] - 1.7) * 0.7\n",
        "        a['part_e_log'] = (a['part_e_log'] - 2.0) * 0.7\n",
        "        a['part_logptrel'] = (a['part_logptrel'] - (-4.7)) * 0.7\n",
        "        a['part_logerel'] = (a['part_logerel'] - (-4.7)) * 0.7\n",
        "        a['part_deltaR'] = (a['part_deltaR'] - 0.2) * 4.0\n",
        "        a['part_d0err'] = _clip(a['part_d0err'], 0, 1)\n",
        "        a['part_dzerr'] = _clip(a['part_dzerr'], 0, 1)\n",
        "\n",
        "    feature_list = {\n",
        "        'pf_points': ['part_deta', 'part_dphi'], # not used in ParT\n",
        "        'pf_features': [\n",
        "            'part_pt_log',\n",
        "            'part_e_log',\n",
        "            'part_logptrel',\n",
        "            'part_logerel',\n",
        "            'part_deltaR',\n",
        "            'part_charge',\n",
        "            'part_isChargedHadron',\n",
        "            'part_isNeutralHadron',\n",
        "            'part_isPhoton',\n",
        "            'part_isElectron',\n",
        "            'part_isMuon',\n",
        "            'part_d0',\n",
        "            'part_d0err',\n",
        "            'part_dz',\n",
        "            'part_dzerr',\n",
        "            'part_deta',\n",
        "            'part_dphi',\n",
        "        ],\n",
        "        'pf_vectors': [\n",
        "            'part_px',\n",
        "            'part_py',\n",
        "            'part_pz',\n",
        "            'part_energy',\n",
        "        ],\n",
        "        'pf_mask': ['part_mask']\n",
        "    }\n",
        "\n",
        "    out = {}\n",
        "    for k, names in feature_list.items():\n",
        "        out[k] = np.stack([_pad(a[n], maxlen=128).to_numpy() for n in names], axis=1)\n",
        "\n",
        "    label_list = ['label_QCD', 'label_Hbb', 'label_Hcc', 'label_Hgg', 'label_H4q', 'label_Hqql', 'label_Zqq', 'label_Wqq', 'label_Tbqq', 'label_Tbl']\n",
        "    out['label'] = np.stack([a[n].to_numpy().astype('int') for n in label_list], axis=1)\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8sdilqQMub7"
      },
      "outputs": [],
      "source": [
        "!pip install 'weaver-core>=0.4'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-7XrQcrCiq0"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1G4A5ddFnNZW"
      },
      "outputs": [],
      "source": [
        "!pip install fastjet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRvfldYELz_9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLf-WRVNm_Qe"
      },
      "outputs": [],
      "source": [
        "''' Particle Transformer (ParT)\n",
        "\n",
        "Paper: \"Particle Transformer for Jet Tagging\" - https://arxiv.org/abs/2202.03772\n",
        "'''\n",
        "import math\n",
        "import random\n",
        "import warnings\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "\n",
        "from typing import Dict, Optional, Tuple\n",
        "#from fairseq import utils\n",
        "#from fairseq.incremental_decoding_utils import with_incremental_state\n",
        "#from fairseq.modules.quant_noise import quant_noise\n",
        "from torch import Tensor, nn\n",
        "from torch.nn import Parameter\n",
        "from weaver.utils.logger import _logger\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "@torch.jit.script\n",
        "def delta_phi(a, b):\n",
        "    return (a - b + math.pi) % (2 * math.pi) - math.pi\n",
        "\n",
        "\n",
        "@torch.jit.script\n",
        "def delta_r2(eta1, phi1, eta2, phi2):\n",
        "    return (eta1 - eta2)**2 + delta_phi(phi1, phi2)**2\n",
        "\n",
        "\n",
        "def to_pt2(x, eps=1e-8):\n",
        "    pt2 = x[:, :2].square().sum(dim=1, keepdim=True)\n",
        "    if eps is not None:\n",
        "        pt2 = pt2.clamp(min=eps)\n",
        "    return pt2\n",
        "\n",
        "\n",
        "def to_m2(x, eps=1e-8):\n",
        "    m2 = x[:, 3:4].square() - x[:, :3].square().sum(dim=1, keepdim=True)\n",
        "    if eps is not None:\n",
        "        m2 = m2.clamp(min=eps)\n",
        "    return m2\n",
        "\n",
        "\n",
        "def atan2(y, x):\n",
        "    sx = torch.sign(x)\n",
        "    sy = torch.sign(y)\n",
        "    pi_part = (sy + sx * (sy ** 2 - 1)) * (sx - 1) * (-math.pi / 2)\n",
        "    atan_part = torch.arctan(y / (x + (1 - sx ** 2))) * sx ** 2\n",
        "    return atan_part + pi_part\n",
        "\n",
        "\n",
        "def to_ptrapphim(x, return_mass=True, eps=1e-8, for_onnx=False):\n",
        "    # x: (N, 4, ...), dim1 : (px, py, pz, E)\n",
        "    px, py, pz, energy = x.split((1, 1, 1, 1), dim=1)\n",
        "    pt = torch.sqrt(to_pt2(x, eps=eps))\n",
        "    # rapidity = 0.5 * torch.log((energy + pz) / (energy - pz))\n",
        "    rapidity = 0.5 * torch.log(1 + (2 * pz) / (energy - pz).clamp(min=1e-20))\n",
        "    phi = (atan2 if for_onnx else torch.atan2)(py, px)\n",
        "    if not return_mass:\n",
        "        return torch.cat((pt, rapidity, phi), dim=1)\n",
        "    else:\n",
        "        m = torch.sqrt(to_m2(x, eps=eps))\n",
        "        return torch.cat((pt, rapidity, phi, m), dim=1)\n",
        "\n",
        "\n",
        "def boost(x, boostp4, eps=1e-8):\n",
        "    # boost x to the rest frame of boostp4\n",
        "    # x: (N, 4, ...), dim1 : (px, py, pz, E)\n",
        "    p3 = -boostp4[:, :3] / boostp4[:, 3:].clamp(min=eps)\n",
        "    b2 = p3.square().sum(dim=1, keepdim=True)\n",
        "    gamma = (1 - b2).clamp(min=eps)**(-0.5)\n",
        "    gamma2 = (gamma - 1) / b2\n",
        "    gamma2.masked_fill_(b2 == 0, 0)\n",
        "    bp = (x[:, :3] * p3).sum(dim=1, keepdim=True)\n",
        "    v = x[:, :3] + gamma2 * bp * p3 + x[:, 3:] * gamma * p3\n",
        "    return v\n",
        "\n",
        "\n",
        "def p3_norm(p, eps=1e-8):\n",
        "    return p[:, :3] / p[:, :3].norm(dim=1, keepdim=True).clamp(min=eps)\n",
        "\n",
        "\n",
        "def pairwise_lv_fts(xi, xj, num_outputs=4, eps=1e-8, for_onnx=False):\n",
        "    pti, rapi, phii = to_ptrapphim(xi, False, eps=None, for_onnx=for_onnx).split((1, 1, 1), dim=1)\n",
        "    ptj, rapj, phij = to_ptrapphim(xj, False, eps=None, for_onnx=for_onnx).split((1, 1, 1), dim=1)\n",
        "\n",
        "    delta = delta_r2(rapi, phii, rapj, phij).sqrt()\n",
        "    lndelta = torch.log(delta.clamp(min=eps))\n",
        "    if num_outputs == 1:\n",
        "        return lndelta\n",
        "\n",
        "    if num_outputs > 1:\n",
        "        ptmin = ((pti <= ptj) * pti + (pti > ptj) * ptj) if for_onnx else torch.minimum(pti, ptj)\n",
        "        lnkt = torch.log((ptmin * delta).clamp(min=eps))\n",
        "        lnz = torch.log((ptmin / (pti + ptj).clamp(min=eps)).clamp(min=eps))\n",
        "        outputs = [lnkt, lnz, lndelta]\n",
        "\n",
        "    if num_outputs > 3:\n",
        "        xij = xi + xj\n",
        "        lnm2 = torch.log(to_m2(xij, eps=eps))\n",
        "        outputs.append(lnm2)\n",
        "\n",
        "    if num_outputs > 4:\n",
        "        lnds2 = torch.log(torch.clamp(-to_m2(xi - xj, eps=None), min=eps))\n",
        "        outputs.append(lnds2)\n",
        "\n",
        "    # the following features are not symmetric for (i, j)\n",
        "    if num_outputs > 5:\n",
        "        xj_boost = boost(xj, xij)\n",
        "        costheta = (p3_norm(xj_boost, eps=eps) * p3_norm(xij, eps=eps)).sum(dim=1, keepdim=True)\n",
        "        outputs.append(costheta)\n",
        "\n",
        "    if num_outputs > 6:\n",
        "        deltarap = rapi - rapj\n",
        "        deltaphi = delta_phi(phii, phij)\n",
        "        outputs += [deltarap, deltaphi]\n",
        "\n",
        "    assert (len(outputs) == num_outputs)\n",
        "    return torch.cat(outputs, dim=1)\n",
        "\n",
        "\n",
        "def build_sparse_tensor(uu, idx, seq_len):\n",
        "    # inputs: uu (N, C, num_pairs), idx (N, 2, num_pairs)\n",
        "    # return: (N, C, seq_len, seq_len)\n",
        "    batch_size, num_fts, num_pairs = uu.size()\n",
        "    idx = torch.min(idx, torch.ones_like(idx) * seq_len)\n",
        "    i = torch.cat((\n",
        "        torch.arange(0, batch_size, device=uu.device).repeat_interleave(num_fts * num_pairs).unsqueeze(0),\n",
        "        torch.arange(0, num_fts, device=uu.device).repeat_interleave(num_pairs).repeat(batch_size).unsqueeze(0),\n",
        "        idx[:, :1, :].expand_as(uu).flatten().unsqueeze(0),\n",
        "        idx[:, 1:, :].expand_as(uu).flatten().unsqueeze(0),\n",
        "    ), dim=0)\n",
        "    return torch.sparse_coo_tensor(\n",
        "        i, uu.flatten(),\n",
        "        size=(batch_size, num_fts, seq_len + 1, seq_len + 1),\n",
        "        device=uu.device).to_dense()[:, :, :seq_len, :seq_len]\n",
        "\n",
        "\n",
        "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
        "    # From https://github.com/rwightman/pytorch-image-models/blob/18ec173f95aa220af753358bf860b16b6691edb2/timm/layers/weight_init.py#L8\n",
        "    r\"\"\"Fills the input Tensor with values drawn from a truncated\n",
        "    normal distribution. The values are effectively drawn from the\n",
        "    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n",
        "    with values outside :math:`[a, b]` redrawn until they are within\n",
        "    the bounds. The method used for generating the random values works\n",
        "    best when :math:`a \\leq \\text{mean} \\leq b`.\n",
        "    Args:\n",
        "        tensor: an n-dimensional `torch.Tensor`\n",
        "        mean: the mean of the normal distribution\n",
        "        std: the standard deviation of the normal distribution\n",
        "        a: the minimum cutoff value\n",
        "        b: the maximum cutoff value\n",
        "    Examples:\n",
        "        >>> w = torch.empty(3, 5)\n",
        "        >>> nn.init.trunc_normal_(w)\n",
        "    \"\"\"\n",
        "    def norm_cdf(x):\n",
        "        # Computes standard normal cumulative distribution function\n",
        "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
        "\n",
        "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
        "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
        "                      \"The distribution of values may be incorrect.\",\n",
        "                      stacklevel=2)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Values are generated by using a truncated uniform distribution and\n",
        "        # then using the inverse CDF for the normal distribution.\n",
        "        # Get upper and lower cdf values\n",
        "        l = norm_cdf((a - mean) / std)\n",
        "        u = norm_cdf((b - mean) / std)\n",
        "\n",
        "        # Uniformly fill tensor with values from [l, u], then translate to\n",
        "        # [2l-1, 2u-1].\n",
        "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
        "\n",
        "        # Use inverse cdf transform for normal distribution to get truncated\n",
        "        # standard normal\n",
        "        tensor.erfinv_()\n",
        "\n",
        "        # Transform to proper mean, std\n",
        "        tensor.mul_(std * math.sqrt(2.))\n",
        "        tensor.add_(mean)\n",
        "\n",
        "        # Clamp to ensure it's in the proper range\n",
        "        tensor.clamp_(min=a, max=b)\n",
        "        return tensor\n",
        "\n",
        "\n",
        "class SequenceTrimmer(nn.Module):\n",
        "\n",
        "    def __init__(self, enabled=False, target=(0.9, 1.02), **kwargs) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "        self.enabled = enabled\n",
        "        self.target = target\n",
        "        self._counter = 0\n",
        "\n",
        "    def forward(self, x, v=None, mask=None, uu=None):\n",
        "        # x: (N, C, P)\n",
        "        # v: (N, 4, P) [px,py,pz,energy]\n",
        "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
        "        # uu: (N, C', P, P)\n",
        "        if mask is None:\n",
        "            mask = torch.ones_like(x[:, :1])\n",
        "        mask = mask.bool()\n",
        "\n",
        "        if self.enabled:\n",
        "            if self._counter < 5:\n",
        "                self._counter += 1\n",
        "            else:\n",
        "                if self.training:\n",
        "                    q = min(1, random.uniform(*self.target))\n",
        "                    maxlen = torch.quantile(mask.type_as(x).sum(dim=-1), q).long()\n",
        "                    rand = torch.rand_like(mask.type_as(x))\n",
        "                    rand.masked_fill_(~mask, -1)\n",
        "                    perm = rand.argsort(dim=-1, descending=True)  # (N, 1, P)\n",
        "                    mask = torch.gather(mask, -1, perm)\n",
        "                    x = torch.gather(x, -1, perm.expand_as(x))\n",
        "                    if v is not None:\n",
        "                        v = torch.gather(v, -1, perm.expand_as(v))\n",
        "                    if uu is not None:\n",
        "                        uu = torch.gather(uu, -2, perm.unsqueeze(-1).expand_as(uu))\n",
        "                        uu = torch.gather(uu, -1, perm.unsqueeze(-2).expand_as(uu))\n",
        "                else:\n",
        "                    maxlen = mask.sum(dim=-1).max()\n",
        "                maxlen = max(maxlen, 1)\n",
        "                if maxlen < mask.size(-1):\n",
        "                    mask = mask[:, :, :maxlen]\n",
        "                    x = x[:, :, :maxlen]\n",
        "                    if v is not None:\n",
        "                        v = v[:, :, :maxlen]\n",
        "                    if uu is not None:\n",
        "                        uu = uu[:, :, :maxlen, :maxlen]\n",
        "\n",
        "        return x, v, mask, uu\n",
        "\n",
        "\n",
        "class Embed(nn.Module):\n",
        "    def __init__(self, input_dim, dims, normalize_input=True, activation='gelu'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_bn = nn.BatchNorm1d(input_dim) if normalize_input else None\n",
        "        module_list = []\n",
        "        for dim in dims:\n",
        "            module_list.extend([\n",
        "                nn.LayerNorm(input_dim),\n",
        "                nn.Linear(input_dim, dim),\n",
        "                nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
        "            ])\n",
        "            input_dim = dim\n",
        "        self.embed = nn.Sequential(*module_list)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.input_bn is not None:\n",
        "            # x: (batch, embed_dim, seq_len)\n",
        "            x = self.input_bn(x)\n",
        "            x = x.permute(2, 0, 1).contiguous()\n",
        "        # x: (seq_len, batch, embed_dim)\n",
        "        return self.embed(x)\n",
        "\n",
        "\n",
        "class PairEmbed(nn.Module):\n",
        "    def __init__(\n",
        "            self, pairwise_lv_dim, pairwise_input_dim, dims,\n",
        "            remove_self_pair=False, use_pre_activation_pair=True, mode='sum',\n",
        "            normalize_input=True, activation='gelu', eps=1e-8,\n",
        "            for_onnx=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.pairwise_lv_dim = pairwise_lv_dim\n",
        "        self.pairwise_input_dim = pairwise_input_dim\n",
        "        self.is_symmetric = (pairwise_lv_dim <= 5) and (pairwise_input_dim == 0)\n",
        "        self.remove_self_pair = remove_self_pair\n",
        "        self.mode = mode\n",
        "        self.for_onnx = for_onnx\n",
        "        self.pairwise_lv_fts = partial(pairwise_lv_fts, num_outputs=pairwise_lv_dim, eps=eps, for_onnx=for_onnx)\n",
        "        self.out_dim = dims[-1]\n",
        "\n",
        "        if self.mode == 'concat':\n",
        "            input_dim = pairwise_lv_dim + pairwise_input_dim\n",
        "            module_list = [nn.BatchNorm1d(input_dim)] if normalize_input else []\n",
        "            for dim in dims:\n",
        "                module_list.extend([\n",
        "                    nn.Conv1d(input_dim, dim, 1),\n",
        "                    nn.BatchNorm1d(dim),\n",
        "                    nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
        "                ])\n",
        "                input_dim = dim\n",
        "            if use_pre_activation_pair:\n",
        "                module_list = module_list[:-1]\n",
        "            self.embed = nn.Sequential(*module_list)\n",
        "        elif self.mode == 'sum':\n",
        "            if pairwise_lv_dim > 0:\n",
        "                input_dim = pairwise_lv_dim\n",
        "                module_list = [nn.BatchNorm1d(input_dim)] if normalize_input else []\n",
        "                for dim in dims:\n",
        "                    module_list.extend([\n",
        "                        nn.Conv1d(input_dim, dim, 1),\n",
        "                        nn.BatchNorm1d(dim),\n",
        "                        nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
        "                    ])\n",
        "                    input_dim = dim\n",
        "                if use_pre_activation_pair:\n",
        "                    module_list = module_list[:-1]\n",
        "                self.embed = nn.Sequential(*module_list)\n",
        "\n",
        "            if pairwise_input_dim > 0:\n",
        "                input_dim = pairwise_input_dim\n",
        "                module_list = [nn.BatchNorm1d(input_dim)] if normalize_input else []\n",
        "                for dim in dims:\n",
        "                    module_list.extend([\n",
        "                        nn.Conv1d(input_dim, dim, 1),\n",
        "                        nn.BatchNorm1d(dim),\n",
        "                        nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
        "                    ])\n",
        "                    input_dim = dim\n",
        "                if use_pre_activation_pair:\n",
        "                    module_list = module_list[:-1]\n",
        "                self.fts_embed = nn.Sequential(*module_list)\n",
        "        else:\n",
        "            raise RuntimeError('`mode` can only be `sum` or `concat`')\n",
        "\n",
        "    def forward(self, x, uu=None):\n",
        "        # x: (batch, v_dim, seq_len)\n",
        "        # uu: (batch, v_dim, seq_len, seq_len)\n",
        "        assert (x is not None or uu is not None)\n",
        "        with torch.no_grad():\n",
        "            if x is not None:\n",
        "                batch_size, _, seq_len = x.size()\n",
        "            else:\n",
        "                batch_size, _, seq_len, _ = uu.size()\n",
        "            if self.is_symmetric and not self.for_onnx:\n",
        "                i, j = torch.tril_indices(seq_len, seq_len, offset=-1 if self.remove_self_pair else 0,\n",
        "                                          device=(x if x is not None else uu).device)\n",
        "                if x is not None:\n",
        "                    x = x.unsqueeze(-1).repeat(1, 1, 1, seq_len)\n",
        "                    xi = x[:, :, i, j]  # (batch, dim, seq_len*(seq_len+1)/2)\n",
        "                    xj = x[:, :, j, i]\n",
        "                    x = self.pairwise_lv_fts(xi, xj)\n",
        "                if uu is not None:\n",
        "                    # (batch, dim, seq_len*(seq_len+1)/2)\n",
        "                    uu = uu[:, :, i, j]\n",
        "            else:\n",
        "                if x is not None:\n",
        "                    x = self.pairwise_lv_fts(x.unsqueeze(-1), x.unsqueeze(-2))\n",
        "                    if self.remove_self_pair:\n",
        "                        i = torch.arange(0, seq_len, device=x.device)\n",
        "                        x[:, :, i, i] = 0\n",
        "                    x = x.view(-1, self.pairwise_lv_dim, seq_len * seq_len)\n",
        "                if uu is not None:\n",
        "                    uu = uu.view(-1, self.pairwise_input_dim, seq_len * seq_len)\n",
        "            if self.mode == 'concat':\n",
        "                if x is None:\n",
        "                    pair_fts = uu\n",
        "                elif uu is None:\n",
        "                    pair_fts = x\n",
        "                else:\n",
        "                    pair_fts = torch.cat((x, uu), dim=1)\n",
        "\n",
        "        if self.mode == 'concat':\n",
        "            elements = self.embed(pair_fts)  # (batch, embed_dim, num_elements)\n",
        "        elif self.mode == 'sum':\n",
        "            if x is None:\n",
        "                elements = self.fts_embed(uu)\n",
        "            elif uu is None:\n",
        "                elements = self.embed(x)\n",
        "            else:\n",
        "                elements = self.embed(x) + self.fts_embed(uu)\n",
        "\n",
        "        if self.is_symmetric and not self.for_onnx:\n",
        "            y = torch.zeros(batch_size, self.out_dim, seq_len, seq_len, dtype=elements.dtype, device=elements.device)\n",
        "            y[:, :, i, j] = elements\n",
        "            y[:, :, j, i] = elements\n",
        "        else:\n",
        "            y = elements.view(-1, self.out_dim, seq_len, seq_len)\n",
        "        return y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, embed_dim=128, num_heads=8, ffn_ratio=4,\n",
        "                 dropout=0.1, attn_dropout=0.1, activation_dropout=0.1,\n",
        "                 add_bias_kv=False, activation='gelu',\n",
        "                 scale_fc=True, scale_attn=True, scale_heads=True, scale_resids=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.ffn_dim = embed_dim * ffn_ratio\n",
        "        self.interaction = None\n",
        "        self.pre_mask_attn_weights = None  # To store attention weights before mask is applied\n",
        "\n",
        "        self.pre_attn_norm = nn.LayerNorm(embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(\n",
        "            embed_dim,\n",
        "            num_heads,\n",
        "            dropout=attn_dropout,\n",
        "            add_bias_kv=add_bias_kv,\n",
        "        )\n",
        "        self.post_attn_norm = nn.LayerNorm(embed_dim) if scale_attn else None\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.pre_fc_norm = nn.LayerNorm(embed_dim)\n",
        "        self.fc1 = nn.Linear(embed_dim, self.ffn_dim)\n",
        "        self.act = nn.GELU() if activation == 'gelu' else nn.ReLU()\n",
        "        self.act_dropout = nn.Dropout(activation_dropout)\n",
        "        self.post_fc_norm = nn.LayerNorm(self.ffn_dim) if scale_fc else None\n",
        "        self.fc2 = nn.Linear(self.ffn_dim, embed_dim)\n",
        "\n",
        "        self.c_attn = nn.Parameter(torch.ones(num_heads), requires_grad=True) if scale_heads else None\n",
        "        self.w_resid = nn.Parameter(torch.ones(embed_dim), requires_grad=True) if scale_resids else None\n",
        "    def getAttention(self):\n",
        "        return self.interaction\n",
        "    def getPreMaskAttention(self):\n",
        "        return self.pre_mask_attn_weights\n",
        "    def forward(self, x, x_cls=None, padding_mask=None, attn_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (Tensor): input to the layer of shape (seq_len, batch, embed_dim)\n",
        "            x_cls (Tensor, optional): class token input to the layer of shape (1, batch, embed_dim)\n",
        "            padding_mask (ByteTensor, optional): binary\n",
        "                ByteTensor of shape (batch, seq_len) where padding\n",
        "                elements are indicated by `1.\n",
        "\n",
        "        Returns:\n",
        "            encoded output of shape (seq_len, batch, embed_dim)\n",
        "        \"\"\"\n",
        "\n",
        "        if x_cls is not None:\n",
        "            with torch.no_grad():\n",
        "                # prepend one element for x_cls: -> (batch, 1+seq_len)\n",
        "                padding_mask = torch.cat((torch.zeros_like(padding_mask[:, :1]), padding_mask), dim=1)\n",
        "            # class attention: https://arxiv.org/pdf/2103.17239.pdf\n",
        "            residual = x_cls\n",
        "            u = torch.cat((x_cls, x), dim=0)  # (seq_len+1, batch, embed_dim)\n",
        "            u = self.pre_attn_norm(u)\n",
        "            x = self.attn(x_cls, u, u, key_padding_mask=padding_mask)[0]  # (1, batch, embed_dim)\n",
        "        else:\n",
        "            residual = x\n",
        "\n",
        "\n",
        "            x = self.pre_attn_norm(x)\n",
        "\n",
        "\n",
        "            x= self.attn(x, x, x, key_padding_mask=padding_mask,\n",
        "                          attn_mask=attn_mask, average_attn_weights=False)[0]  # (seq_len, batch, embed_dim)\n",
        "            y= self.attn(x, x, x, key_padding_mask=padding_mask,\n",
        "                          attn_mask=attn_mask, average_attn_weights=False)[1]\n",
        "            self.interaction = y\n",
        "\n",
        "\n",
        "        if self.c_attn is not None:\n",
        "            tgt_len = x.size(0)\n",
        "            x = x.view(tgt_len, -1, self.num_heads, self.head_dim)\n",
        "            x = torch.einsum('tbhd,h->tbdh', x, self.c_attn)\n",
        "            x = x.reshape(tgt_len, -1, self.embed_dim)\n",
        "        if self.post_attn_norm is not None:\n",
        "            x = self.post_attn_norm(x)\n",
        "        x = self.dropout(x)\n",
        "        x += residual\n",
        "\n",
        "        residual = x\n",
        "        x = self.pre_fc_norm(x)\n",
        "        x = self.act(self.fc1(x))\n",
        "        x = self.act_dropout(x)\n",
        "        if self.post_fc_norm is not None:\n",
        "            x = self.post_fc_norm(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        if self.w_resid is not None:\n",
        "            residual = torch.mul(self.w_resid, residual)\n",
        "        x += residual\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ParticleTransformer(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 num_classes=10,\n",
        "                 # network configurations\n",
        "                 pair_input_dim=4,\n",
        "                 pair_extra_dim=0,\n",
        "                 remove_self_pair=False,\n",
        "                 use_pre_activation_pair=True,\n",
        "                 embed_dims=[64, 64, 64],\n",
        "                 pair_embed_dims=[32, 32, 32],\n",
        "                 num_heads=1,\n",
        "                 num_layers=1,\n",
        "                 num_cls_layers=1,\n",
        "                 block_params=None,\n",
        "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
        "                 fc_params=[],\n",
        "                 activation='gelu',\n",
        "                 # misc\n",
        "                 trim=True,\n",
        "                 for_inference=False,\n",
        "                 use_amp=False,\n",
        "                 **kwargs) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
        "        self.attention_matrix = []\n",
        "        self.for_inference = for_inference\n",
        "        self.use_amp = use_amp\n",
        "        embed_dim = embed_dims[-1] if len(embed_dims) > 0 else input_dim\n",
        "        default_cfg = dict(embed_dim=embed_dim, num_heads=num_heads, ffn_ratio=4,\n",
        "                           dropout=0.1, attn_dropout=0.1, activation_dropout=0.1,\n",
        "                           add_bias_kv=False, activation=activation,\n",
        "                           scale_fc=True, scale_attn=True, scale_heads=True, scale_resids=True)\n",
        "        self.pairMatrixes = []\n",
        "\n",
        "        cfg_block = copy.deepcopy(default_cfg)\n",
        "        if block_params is not None:\n",
        "            cfg_block.update(block_params)\n",
        "        _logger.info('cfg_block: %s' % str(cfg_block))\n",
        "\n",
        "        cfg_cls_block = copy.deepcopy(default_cfg)\n",
        "        if cls_block_params is not None:\n",
        "            cfg_cls_block.update(cls_block_params)\n",
        "        _logger.info('cfg_cls_block: %s' % str(cfg_cls_block))\n",
        "\n",
        "        self.pair_extra_dim = pair_extra_dim\n",
        "        self.embed = Embed(input_dim, embed_dims, activation=activation) if len(embed_dims) > 0 else nn.Identity()\n",
        "        self.pair_embed = PairEmbed(\n",
        "            pair_input_dim, pair_extra_dim, pair_embed_dims + [cfg_block['num_heads']],\n",
        "            remove_self_pair=remove_self_pair, use_pre_activation_pair=use_pre_activation_pair,\n",
        "            for_onnx=for_inference) if pair_embed_dims is not None and pair_input_dim + pair_extra_dim > 0 else None\n",
        "        self.blocks = nn.ModuleList([Block(**cfg_block) for _ in range(num_layers)])\n",
        "        self.cls_blocks = nn.ModuleList([Block(**cfg_cls_block) for _ in range(num_cls_layers)])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.interactionMatrix = None\n",
        "\n",
        "        if fc_params is not None:\n",
        "            fcs = []\n",
        "            in_dim = embed_dim\n",
        "            for out_dim, drop_rate in fc_params:\n",
        "                fcs.append(nn.Sequential(nn.Linear(in_dim, out_dim), nn.ReLU(), nn.Dropout(drop_rate)))\n",
        "                in_dim = out_dim\n",
        "            fcs.append(nn.Linear(in_dim, num_classes))\n",
        "            self.fc = nn.Sequential(*fcs)\n",
        "        else:\n",
        "            self.fc = None\n",
        "\n",
        "        # init\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim), requires_grad=True)\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'cls_token', }\n",
        "\n",
        "    def getAttention(self):\n",
        "        return self.attention_matrix\n",
        "\n",
        "    def getInteraction(self):\n",
        "        return self.interactionMatrix\n",
        "\n",
        "    def getPairs(self):\n",
        "        return self.pairMatrixes\n",
        "\n",
        "\n",
        "    def forward(self, x, v=None, mask=None, uu=None, uu_idx=None):\n",
        "        # x: (N, C, P)\n",
        "        # v: (N, 4, P) [px,py,pz,energy]\n",
        "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
        "        # for pytorch: uu (N, C', num_pairs), uu_idx (N, 2, num_pairs)\n",
        "        # for onnx: uu (N, C', P, P), uu_idx=None\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if not self.for_inference:\n",
        "                if uu_idx is not None:\n",
        "                    uu = build_sparse_tensor(uu, uu_idx, x.size(-1))\n",
        "            x, v, mask, uu = self.trimmer(x, v, mask, uu)\n",
        "            padding_mask = ~mask.squeeze(1)  # (N, P)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
        "            # input embedding\n",
        "            x = self.embed(x).masked_fill(~mask.permute(2, 0, 1), 0)  # (P, N, C)\n",
        "            attn_mask = None\n",
        "            if (v is not None or uu is not None) and self.pair_embed is not None:\n",
        "                attn_mask = self.pair_embed(v, uu).view(-1, v.size(-1), v.size(-1))  # (N*num_heads, P, P)\n",
        "\n",
        "            # transform\n",
        "            #num = 0\n",
        "            for block in self.blocks:\n",
        "\n",
        "                x = block(x, x_cls=None, padding_mask=padding_mask, attn_mask=attn_mask)\n",
        "                self.interactionMatrix = attn_mask\n",
        "                #if num == 0 :\n",
        "                self.attention_matrix.append(block.interaction)\n",
        "                #num = num + 1\n",
        "\n",
        "            # extract class token\n",
        "            cls_tokens = self.cls_token.expand(1, x.size(1), -1)  # (1, N, C)\n",
        "            for block in self.cls_blocks:\n",
        "                cls_tokens = block(x, x_cls=cls_tokens, padding_mask=padding_mask)\n",
        "\n",
        "            x_cls = self.norm(cls_tokens).squeeze(0)\n",
        "\n",
        "            # fc\n",
        "            if self.fc is None:\n",
        "                return x_cls\n",
        "            output = self.fc(x_cls)\n",
        "            if self.for_inference:\n",
        "                output = torch.softmax(output, dim=1)\n",
        "\n",
        "\n",
        "            return output\n",
        "\n",
        "class ParticleTransformerTagger(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 pf_input_dim,\n",
        "                 sv_input_dim,\n",
        "                 num_classes=None,\n",
        "                 # network configurations\n",
        "                 pair_input_dim=4,\n",
        "                 pair_extra_dim=0,\n",
        "                 remove_self_pair=False,\n",
        "                 use_pre_activation_pair=True,\n",
        "                 embed_dims=[128, 512, 128],\n",
        "                 pair_embed_dims=[64, 64, 64],\n",
        "                 num_heads=8,\n",
        "                 num_layers=8,\n",
        "                 num_cls_layers=2,\n",
        "                 block_params=None,\n",
        "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
        "                 fc_params=[],\n",
        "                 activation='gelu',\n",
        "                 # misc\n",
        "                 trim=True,\n",
        "                 for_inference=False,\n",
        "                 use_amp=False,\n",
        "                 **kwargs) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.use_amp = use_amp\n",
        "\n",
        "        self.pf_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
        "        self.sv_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
        "\n",
        "        self.pf_embed = Embed(pf_input_dim, embed_dims, activation=activation)\n",
        "        self.sv_embed = Embed(sv_input_dim, embed_dims, activation=activation)\n",
        "\n",
        "        self.part = ParticleTransformer(input_dim=embed_dims[-1],\n",
        "                                        num_classes=num_classes,\n",
        "                                        # network configurations\n",
        "                                        pair_input_dim=pair_input_dim,\n",
        "                                        pair_extra_dim=pair_extra_dim,\n",
        "                                        remove_self_pair=remove_self_pair,\n",
        "                                        use_pre_activation_pair=use_pre_activation_pair,\n",
        "                                        embed_dims=[],\n",
        "                                        pair_embed_dims=pair_embed_dims,\n",
        "                                        num_heads=num_heads,\n",
        "                                        num_layers=num_layers,\n",
        "                                        num_cls_layers=num_cls_layers,\n",
        "                                        block_params=block_params,\n",
        "                                        cls_block_params=cls_block_params,\n",
        "                                        fc_params=fc_params,\n",
        "                                        activation=activation,\n",
        "                                        # misc\n",
        "                                        trim=False,\n",
        "                                        for_inference=for_inference,\n",
        "                                        use_amp=use_amp)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'part.cls_token', }\n",
        "\n",
        "    def forward(self, pf_x, pf_v=None, pf_mask=None, sv_x=None, sv_v=None, sv_mask=None):\n",
        "        # x: (N, C, P)\n",
        "        # v: (N, 4, P) [px,py,pz,energy]\n",
        "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pf_x, pf_v, pf_mask, _ = self.pf_trimmer(pf_x, pf_v, pf_mask)\n",
        "            sv_x, sv_v, sv_mask, _ = self.sv_trimmer(sv_x, sv_v, sv_mask)\n",
        "            v = torch.cat([pf_v, sv_v], dim=2)\n",
        "            mask = torch.cat([pf_mask, sv_mask], dim=2)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
        "            pf_x = self.pf_embed(pf_x)  # after embed: (seq_len, batch, embed_dim)\n",
        "            sv_x = self.sv_embed(sv_x)\n",
        "            x = torch.cat([pf_x, sv_x], dim=0)\n",
        "\n",
        "            return self.part(x, v, mask)\n",
        "\n",
        "\n",
        "class ParticleTransformerTaggerWithExtraPairFeatures(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 pf_input_dim,\n",
        "                 sv_input_dim,\n",
        "                 num_classes=None,\n",
        "                 # network configurations\n",
        "                 pair_input_dim=4,\n",
        "                 pair_extra_dim=0,\n",
        "                 remove_self_pair=False,\n",
        "                 use_pre_activation_pair=True,\n",
        "                 embed_dims=[128, 512, 128],\n",
        "                 pair_embed_dims=[64, 64, 64],\n",
        "                 num_heads=8,\n",
        "                 num_layers=8,\n",
        "                 num_cls_layers=2,\n",
        "                 block_params=None,\n",
        "                 cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
        "                 fc_params=[],\n",
        "                 activation='gelu',\n",
        "                 # misc\n",
        "                 trim=True,\n",
        "                 for_inference=False,\n",
        "                 use_amp=False,\n",
        "                 **kwargs) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.use_amp = use_amp\n",
        "        self.for_inference = for_inference\n",
        "\n",
        "        self.pf_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
        "        self.sv_trimmer = SequenceTrimmer(enabled=trim and not for_inference)\n",
        "\n",
        "        self.pf_embed = Embed(pf_input_dim, embed_dims, activation=activation)\n",
        "        self.sv_embed = Embed(sv_input_dim, embed_dims, activation=activation)\n",
        "\n",
        "        self.part = ParticleTransformer(input_dim=embed_dims[-1],\n",
        "                                        num_classes=num_classes,\n",
        "                                        # network configurations\n",
        "                                        pair_input_dim=pair_input_dim,\n",
        "                                        pair_extra_dim=pair_extra_dim,\n",
        "                                        remove_self_pair=remove_self_pair,\n",
        "                                        use_pre_activation_pair=use_pre_activation_pair,\n",
        "                                        embed_dims=[],\n",
        "                                        pair_embed_dims=pair_embed_dims,\n",
        "                                        num_heads=num_heads,\n",
        "                                        num_layers=num_layers,\n",
        "                                        num_cls_layers=num_cls_layers,\n",
        "                                        block_params=block_params,\n",
        "                                        cls_block_params=cls_block_params,\n",
        "                                        fc_params=fc_params,\n",
        "                                        activation=activation,\n",
        "                                        # misc\n",
        "                                        trim=False,\n",
        "                                        for_inference=for_inference,\n",
        "                                        use_amp=use_amp)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'part.cls_token', }\n",
        "\n",
        "    def forward(self, pf_x, pf_v=None, pf_mask=None, sv_x=None, sv_v=None, sv_mask=None, pf_uu=None, pf_uu_idx=None):\n",
        "        # x: (N, C, P)\n",
        "        # v: (N, 4, P) [px,py,pz,energy]\n",
        "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if not self.for_inference:\n",
        "                if pf_uu_idx is not None:\n",
        "                    pf_uu = build_sparse_tensor(pf_uu, pf_uu_idx, pf_x.size(-1))\n",
        "\n",
        "            pf_x, pf_v, pf_mask, pf_uu = self.pf_trimmer(pf_x, pf_v, pf_mask, pf_uu)\n",
        "            sv_x, sv_v, sv_mask, _ = self.sv_trimmer(sv_x, sv_v, sv_mask)\n",
        "            v = torch.cat([pf_v, sv_v], dim=2)\n",
        "            mask = torch.cat([pf_mask, sv_mask], dim=2)\n",
        "            uu = torch.zeros(v.size(0), pf_uu.size(1), v.size(2), v.size(2), dtype=v.dtype, device=v.device)\n",
        "            uu[:, :, :pf_x.size(2), :pf_x.size(2)] = pf_uu\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
        "            pf_x = self.pf_embed(pf_x)  # after embed: (seq_len, batch, embed_dim)\n",
        "            sv_x = self.sv_embed(sv_x)\n",
        "            x = torch.cat([pf_x, sv_x], dim=0)\n",
        "\n",
        "            return self.part(x, v, mask, uu)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SN4AQK66t4u2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "def plot_attention(detaCH, dphiCH, ptCH, detaNH, dphiNH, ptNH,\n",
        "                   detaPhoton, dphiPhoton, ptPhoton,\n",
        "                   detaElectron, dphiElectron, ptElectron,\n",
        "                   detaMuon, dphiMuon, ptMuon,\n",
        "                   pt, eta, phi, attention_matrix, layer_number, subjets,\n",
        "                   plot_single_head=False, head_to_plot=0, show_subjet=None):\n",
        "    # Check if attention_matrix is a PyTorch tensor and convert it\n",
        "    if isinstance(attention_matrix, torch.Tensor):\n",
        "        attention_matrix = attention_matrix.numpy()\n",
        "\n",
        "    num_heads, n, _ = attention_matrix.shape\n",
        "\n",
        "    # Determine how many heads to plot\n",
        "    if plot_single_head:\n",
        "        heads_to_plot = [head_to_plot]\n",
        "        fig, axes = plt.subplots(1, 1, figsize=(8, 8))  # Only one plot, single head\n",
        "        axes = [axes]  # Wrap single axis in a list to maintain consistency\n",
        "    else:\n",
        "        heads_to_plot = range(num_heads)\n",
        "        num_cols = 4\n",
        "        num_rows = int(np.ceil(len(heads_to_plot) / num_cols))\n",
        "        fig, axes = plt.subplots(num_rows, num_cols, figsize=(25, 6 * num_rows))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "    # Normalize the pt values for color mapping\n",
        "    norm_pt = mcolors.Normalize(vmin=pt.min(), vmax=pt.max())\n",
        "    cmap_pt = plt.cm.plasma\n",
        "\n",
        "    # Normalize the attention values for color mapping\n",
        "    norm_attn = mcolors.Normalize(vmin=attention_matrix.min(), vmax=attention_matrix.max())\n",
        "    cmap_attn = plt.cm.Greys\n",
        "\n",
        "    # Define a list of sequential colormaps for subjets\n",
        "    colormaps = [\n",
        "        plt.cm.Blues, plt.cm.Oranges, plt.cm.Greens,\n",
        "        plt.cm.Reds, plt.cm.Purples, plt.cm.Greys\n",
        "    ]\n",
        "\n",
        "    # Ensure we have enough colormaps for the number of subjets\n",
        "    num_subjets = np.max(subjets) + 1\n",
        "    assert num_subjets <= len(colormaps), \"Not enough colormaps defined for the number of subjets.\"\n",
        "\n",
        "    # Assign a distinct colormap to each subjet\n",
        "    subjet_colors = [colormaps[i % len(colormaps)] for i in range(num_subjets)]\n",
        "\n",
        "    # Define distinct colors for connection lines between subjets\n",
        "    from itertools import combinations\n",
        "    color_palette = list(mcolors.TABLEAU_COLORS.values())[:num_subjets * (num_subjets + 1) // 2]\n",
        "    connection_colors = {}\n",
        "    color_idx = 0\n",
        "\n",
        "    for i in range(num_subjets):\n",
        "        # Use the primary color of the subjet's colormap for self-connections\n",
        "        primary_color = subjet_colors[i](0.5)  # Use the midpoint of the colormap\n",
        "        connection_colors[(i, i)] = primary_color\n",
        "\n",
        "    for i, j in combinations(range(num_subjets), 2):\n",
        "        connection_colors[(i, j)] = color_palette[color_idx]\n",
        "        connection_colors[(j, i)] = color_palette[color_idx]\n",
        "        color_idx += 1\n",
        "\n",
        "    # Filter the attention matrix based on the show_subjet\n",
        "    valid_indices = range(n)\n",
        "    if show_subjet is not None:\n",
        "        valid_indices = [i for i in range(n) if subjets[i] == show_subjet]\n",
        "        attention_matrix_filtered = np.zeros_like(attention_matrix)\n",
        "        for head in range(num_heads):\n",
        "            for i in valid_indices:\n",
        "                for j in range(n):\n",
        "                    if subjets[j] == show_subjet or subjets[j] != -1:\n",
        "                        attention_matrix_filtered[head, i, j] = attention_matrix[head, i, j]\n",
        "                        attention_matrix_filtered[head, j, i] = attention_matrix[head, j, i]\n",
        "    else:\n",
        "        attention_matrix_filtered = attention_matrix\n",
        "\n",
        "    for head_idx, head in enumerate(heads_to_plot):\n",
        "        ax = axes[head_idx]\n",
        "        attn = attention_matrix_filtered[head]\n",
        "\n",
        "        # Gather and sort attention pairs by value\n",
        "        attention_pairs = [(attn[i, j], i, j) for i in range(n) for j in range(n) if i != j and attn[i, j] > 0]\n",
        "        attention_pairs.sort()  # Sort by attention value\n",
        "\n",
        "        # Plot attention lines from lowest to highest value (light to dark)\n",
        "        for value, i, j in attention_pairs:\n",
        "            alpha = 1 * norm_attn(value)\n",
        "\n",
        "            color = connection_colors.get((subjets[i], subjets[j]), 'black')\n",
        "            if subjets[i] == subjets[j]:\n",
        "                linestyle = 'solid'\n",
        "                linewidth = 1.5\n",
        "            elif show_subjet is not None and (subjets[i] == show_subjet or subjets[j] == show_subjet):\n",
        "                linestyle = 'dotted'\n",
        "                linewidth = 1.0\n",
        "            else:\n",
        "                linestyle = 'dashed'\n",
        "                linewidth = 2.5\n",
        "\n",
        "            ax.plot([eta[i], eta[j]], [phi[i], phi[j]],\n",
        "                    color=color, alpha=alpha, zorder=1, linewidth=linewidth, linestyle=linestyle)\n",
        "\n",
        "        # Function to get alpha, size, and edge width based on pt\n",
        "        def get_properties(index):\n",
        "            alpha = 0.5 + 0.5 * norm_pt(pt[index])  # Minimum opacity of 0.6\n",
        "            size = 50 + 150 * norm_pt(pt[index])  # Increase size for higher pt\n",
        "            edge_width = 0.5 + 2.0 * norm_pt(pt[index])  # Increase edge width for higher pt\n",
        "            return alpha, size, edge_width\n",
        "\n",
        "        # Function to plot particles with different shapes\n",
        "        def plot_particles(deta, dphi, pt_values, marker, label, subjet_indices):\n",
        "            for i in range(len(deta)):\n",
        "                alpha, size, edge_width = get_properties(i)\n",
        "                color = cmap_pt(norm_pt(pt_values[i]))  # Color by pt\n",
        "                edge_color = subjet_colors[subjet_indices[i]](0.5)  # Edge color by subjet\n",
        "                ax.scatter(deta[i], dphi[i], c=[color], alpha=alpha, s=size, zorder=3, marker=marker,\n",
        "                           edgecolors='black', linewidths=edge_width, label=label if i == 0 else \"\")\n",
        "\n",
        "        # Scatter plot with color by pt and outline color by subjet\n",
        "        plot_particles(detaCH, dphiCH, ptCH, '^', 'Charged Hadron', subjets)\n",
        "        plot_particles(detaNH, dphiNH, ptNH, 'v', 'Neutral Hadron', subjets)\n",
        "        plot_particles(detaPhoton, dphiPhoton, ptPhoton, 'o', 'Photon', subjets)\n",
        "        plot_particles(detaElectron, dphiElectron, ptElectron, 'P', 'Electron', subjets)\n",
        "        plot_particles(detaMuon, dphiMuon, ptMuon, 'X', 'Muon', subjets)\n",
        "\n",
        "        ax.set_title(f'Layer {layer_number}: Head {head + 1}')\n",
        "        ax.set_xlabel('eta')\n",
        "        ax.set_ylabel('phi')\n",
        "\n",
        "    # Remove extra subplots if plotting only one head\n",
        "    if not plot_single_head:\n",
        "        for i in range(len(heads_to_plot), num_rows * num_cols):\n",
        "            fig.delaxes(axes[i])\n",
        "\n",
        "    fig.tight_layout(pad=3.0, w_pad=3.0, h_pad=3.0)\n",
        "\n",
        "    # Add colorbars for pt and attention weights\n",
        "    cbar_pt = fig.colorbar(plt.cm.ScalarMappable(norm=norm_pt, cmap=cmap_pt), ax=axes[:len(heads_to_plot)],\n",
        "                           shrink=0.6, aspect=20, label='Pt')\n",
        "    cbar_attn = fig.colorbar(plt.cm.ScalarMappable(norm=norm_attn, cmap=cmap_attn), ax=axes[:len(heads_to_plot)],\n",
        "                             shrink=0.6, aspect=20, label='Attention Weight', orientation='horizontal', pad=0.05)\n",
        "\n",
        "    # Add legend for connection colors\n",
        "    unique_connections = {tuple(sorted((key[0], key[1]))): value for key, value in connection_colors.items()}\n",
        "    handles_conn = [plt.Line2D([0], [0], color=color, lw=2.5 if i != j else 1.5, linestyle='dashed' if i != j else 'solid') for (i, j), color in unique_connections.items()]\n",
        "    labels_conn = [f'Connection {i}-{j}' for (i, j) in unique_connections.keys()]\n",
        "    legend_conn = fig.legend(handles_conn, labels_conn, loc='upper right', title='Connection Colors', fontsize=12, title_fontsize=14)\n",
        "\n",
        "    # Add legend for particle shapes\n",
        "    handles_shapes = [plt.Line2D([0], [0], color='k', marker='^', linestyle='', markersize=10, label='Charged Hadron'),\n",
        "                      plt.Line2D([0], [0], color='k', marker='v', linestyle='', markersize=10, label='Neutral Hadron'),\n",
        "                      plt.Line2D([0], [0], color='k', marker='o', linestyle='', markersize=10, label='Photon'),\n",
        "                      plt.Line2D([0], [0], color='k', marker='P', linestyle='', markersize=10, label='Electron'),\n",
        "                      plt.Line2D([0], [0], color='k', marker='X', linestyle='', markersize=10, label='Muon')]\n",
        "    legend_shapes = fig.legend(handles_shapes, [handle.get_label() for handle in handles_shapes], loc='lower right', title='Particle Shapes', fontsize=12, title_fontsize=14)\n",
        "\n",
        "    # Add all legends to the figure\n",
        "    fig.add_artist(legend_conn)\n",
        "    fig.add_artist(legend_shapes)\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddpH_iMzlv6U"
      },
      "outputs": [],
      "source": [
        "!pip install 'weaver-core>=0.4'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kn_MKUD7kFz"
      },
      "outputs": [],
      "source": [
        "label_list = ['label_QCD', 'label_Hbb', 'label_Hcc', 'label_Hgg', 'label_H4q', 'label_Hqql', 'label_Zqq', 'label_Wqq', 'label_Tbqq', 'label_Tbl']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90ANRWWfls_q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UC3uIQ37kF0"
      },
      "outputs": [],
      "source": [
        "def find_indexes(input_list):\n",
        "    return [index for index, value in enumerate(input_list) if value == 4]\n",
        "\n",
        "input_list = list(np.argmax(labels,axis=1))\n",
        "indexes = find_indexes(input_list)\n",
        "print(indexes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baKujaeb7kF0"
      },
      "outputs": [],
      "source": [
        "\n",
        "pf_features = np.load('/content/drive/MyDrive/networks/SMALLpfFeat.npy')\n",
        "pf_vectors = np.load('/content/drive/MyDrive/networks/SMALLpfVectors.npy')\n",
        "pf_mask = np.load('/content/drive/MyDrive/networks/SMALLpfMask.npy')\n",
        "pf_points = np.load('/content/drive/MyDrive/networks/SMALLpfPoints.npy')\n",
        "labels = np.load('/content/drive/MyDrive/networks/SMALLlabels.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sbg-Hm_pmIZu"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUexAkySmdg_"
      },
      "outputs": [],
      "source": [
        "jet = 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "head = 0\n",
        "layer = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_MlWekC7kF0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class ParticleTransformerWrapper(torch.nn.Module):\n",
        "    def __init__(self, **kwargs) -> None:\n",
        "        super().__init__()\n",
        "        self.mod = ParticleTransformer(**kwargs)\n",
        "        self.attention_matrix = None\n",
        "        self.interactionMatrix = None\n",
        "        self.pre_mask_attention_matrices = []\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'mod.cls_token', }\n",
        "\n",
        "    def forward(self, points, features, lorentz_vectors, mask):\n",
        "        output = self.mod(features, v=lorentz_vectors, mask=mask)\n",
        "        self.attention_matrix = self.mod.getAttention()\n",
        "        self.interactionMatrix = self.mod.getInteraction()\n",
        "        #self.pre_mask_attention_matrices = self.get_pre_mask_attention_matrices()\n",
        "        return output\n",
        "\n",
        "    def get_attention_matrix(self):\n",
        "        return self.attention_matrix\n",
        "\n",
        "    def get_interactionMatrix(self):\n",
        "        return self.interactionMatrix\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_model(**kwargs):\n",
        "\n",
        "    cfg = dict(\n",
        "        input_dim=17,\n",
        "        num_classes=10,\n",
        "        # network configurations\n",
        "        pair_input_dim=4,\n",
        "        use_pre_activation_pair=False,\n",
        "        embed_dims=[128, 512, 128],\n",
        "        pair_embed_dims=[64, 64, 64],\n",
        "        num_heads=8,\n",
        "        num_layers=8,\n",
        "        num_cls_layers=2,\n",
        "        block_params=None,\n",
        "        cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
        "        fc_params=[],\n",
        "        activation='gelu',\n",
        "        # misc\n",
        "        trim=True,\n",
        "        for_inference=False,\n",
        "    )\n",
        "    cfg.update(**kwargs)\n",
        "\n",
        "    model = ParticleTransformerWrapper(**cfg)\n",
        "\n",
        "    model_info = {\n",
        "\n",
        "    }\n",
        "\n",
        "    return model, model_info\n",
        "\n",
        "\n",
        "def get_loss(data_config, **kwargs):\n",
        "    return torch.nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6Rc6T-U7kF1"
      },
      "outputs": [],
      "source": [
        "model = get_model()\n",
        "state_dict = torch.load('/content/drive/MyDrive/networks/ParT_full.pt', map_location=torch.device('cpu'))\n",
        "model[0].load_state_dict(state_dict)\n",
        "pf_features = pf_features[200:300]\n",
        "pf_vectors = pf_vectors[200:300]\n",
        "pf_mask = pf_mask[200:300]\n",
        "pf_points = pf_points[200:300]\n",
        "labels = labels[200:300]\n",
        "model[0].eval()\n",
        "with torch.no_grad():\n",
        "    y_pred= model[0](torch.from_numpy(pf_points),torch.from_numpy(pf_features),torch.from_numpy(pf_vectors),torch.from_numpy(pf_mask))\n",
        "attention = model[0].get_attention_matrix()\n",
        "interaction = model[0].get_interactionMatrix()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy"
      ],
      "metadata": {
        "id": "wuN-ywAnN4xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred.shape"
      ],
      "metadata": {
        "id": "3OsAZM_0xh0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmaxed = scipy.special.softmax(y_pred, axis = -1)"
      ],
      "metadata": {
        "id": "LtcXMqqCN0XA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "id": "TAPGz-vexlRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in softmaxed:\n",
        "  print(x)"
      ],
      "metadata": {
        "id": "dzprwDUVxc7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 0\n",
        "for x in softmaxed:\n",
        "  if np.argmax(x) != np.argmax(labels[index]):\n",
        "    print(index)\n",
        "    print('        label = ' + str(np.argmax(labels[index])) + ' prediction: ' + str(np.argmax(x)) + ' ' + str(x))\n",
        "  index = index + 1"
      ],
      "metadata": {
        "id": "aT7G_r4NONY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuDdK8pcZu0-"
      },
      "outputs": [],
      "source": [
        "model = get_model()\n",
        "state_dict = torch.load('/content/drive/MyDrive/networks/ParT_full.pt', map_location=torch.device('cpu'))\n",
        "model[0].load_state_dict(state_dict)\n",
        "pf_features = pf_features[0:500]\n",
        "pf_vectors = pf_vectors[0:500]\n",
        "pf_mask = pf_mask[0:500]\n",
        "pf_points = pf_points[0:500]\n",
        "labels = labels[0:500]\n",
        "model[0].eval()\n",
        "with torch.no_grad():\n",
        "    y_pred= model[0](torch.from_numpy(pf_points),torch.from_numpy(pf_features),torch.from_numpy(pf_vectors),torch.from_numpy(pf_mask))\n",
        "attentionU = model[0].get_attention_matrix()\n",
        "interactionU = model[0].get_interactionMatrix()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPmV6HbhLJSW"
      },
      "outputs": [],
      "source": [
        "jet = 9\n",
        "number = jet\n",
        "num = 0\n",
        "for b in np.squeeze(pf_mask[number]):\n",
        "    if b == 0:\n",
        "        break\n",
        "    num = num + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLL0RE7-amTu"
      },
      "outputs": [],
      "source": [
        "import mplhep as hep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ED5C_gTw0wHs"
      },
      "outputs": [],
      "source": [
        "\n",
        "hep.style.use(hep.style.ROOT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Heb75417K92_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming the following variables are already defined\n",
        "# Example dummy data for context:\n",
        "# attention = np.random.rand(12, 128, 64, 64)  # A random tensor for illustration purposes\n",
        "# num = 50  # You can replace this with your actual value\n",
        "\n",
        "# Plotting the attention matrix with the origin in the lower left\n",
        "plt.figure(figsize=(8, 6))  # Adjust figure size as needed\n",
        "\n",
        "# Visualizing a specific part of the attention matrix\n",
        "plt.imshow(attention[9][0][3, 0:num, 0:num], origin='lower', cmap='viridis')\n",
        "\n",
        "# Adding colorbar\n",
        "plt.colorbar()\n",
        "\n",
        "# Set the x and y ticks to integer values\n",
        "plt.xticks(np.arange(0, num, step=5))  # Set x-axis ticks as integers with step size of 5\n",
        "plt.yticks(np.arange(0, num, step=5))  # Same for y-axis\n",
        "plt.title(r'$t \\rightarrow bqq^{\\prime}$ Attention Heat Map')\n",
        "\n",
        "# Using LaTeX for the title in particle decay process format\n",
        "\n",
        "# Save the plot to a file\n",
        "plt.savefig('/content/drive/MyDrive/networks/Plots/HeatmapTbqq.pdf', bbox_inches=\"tight\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1Okd0nd7kF1"
      },
      "outputs": [],
      "source": [
        "interaction = model[0].get_interactionMatrix()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FEfKMNdqq3c"
      },
      "outputs": [],
      "source": [
        "interaction.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBzdbrQ57kF1"
      },
      "outputs": [],
      "source": [
        "interaction= np.reshape(interaction, (attention[0].shape))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbUn393LONhm"
      },
      "outputs": [],
      "source": [
        "intstack = []\n",
        "for x in range(8):\n",
        "  intstack.append(interaction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tilNOyDWmHX"
      },
      "outputs": [],
      "source": [
        "intstack = np.stack(intstack)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENfwKfbZWqRq"
      },
      "outputs": [],
      "source": [
        "intstack = intstack.transpose(1,0,2,3,4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKwlYKAuNf3H"
      },
      "outputs": [],
      "source": [
        "atstack = np.stack(attention).transpose(1,0,2,3,4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMDKYa1lNrur"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "epsilon = 1e-10\n",
        "log_softmaxed_attn = np.log(atstack + 1e-10)\n",
        "\n",
        "\n",
        "pre_attn_matrix= log_softmaxed_attn - intstack\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAjoakyxjUZ3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCJ08_T1W062"
      },
      "outputs": [],
      "source": [
        "attentionOnly = pre_attn_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KwTp4-wfufr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSvNkuS1fabF"
      },
      "outputs": [],
      "source": [
        "attentionOnly.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_KO2FcmxAdJ"
      },
      "outputs": [],
      "source": [
        "jet = 9\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "head = 0\n",
        "layer = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSmKSSHe6PCa"
      },
      "outputs": [],
      "source": [
        "\n",
        "number = jet\n",
        "num = 0\n",
        "for b in np.squeeze(pf_mask[number]):\n",
        "    if b == 0:\n",
        "        break\n",
        "    num = num + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35zXYrjOodSU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle as pkl\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import fastjet\n",
        "\n",
        "\n",
        "def get_subjets(px, py, pz, e, N_SUBJETS=3, JET_ALGO=\"kt\", jet_radius=0.8):\n",
        "    \"\"\"\n",
        "    Declusters a jet into exactly N_SUBJETS using the JET_ALGO and jet_radius provided.\n",
        "\n",
        "    Args:\n",
        "        px [np.ndarray]: NumPy array of shape ``[num_particles]`` containing the px of each particle inside the jet\n",
        "        py [np.ndarray]: NumPy array of shape ``[num_particles]`` containing the py of each particle inside the jet\n",
        "        pz [np.ndarray]: NumPy array of shape ``[num_particles]`` containing the pz of each particle inside the jet\n",
        "        e [np.ndarray]: NumPy array of shape ``[num_particles]`` containing the e of each particle inside the jet\n",
        "        N_SUBJETS [int]: Number of subjets to decluster the jet into\n",
        "            (default is 3)\n",
        "        JET_ALGO [str]: The jet declustering algorithm to use. Choices are [\"CA\", \"kt\", \"antikt\"]\n",
        "            (default is \"CA\")\n",
        "        jet_radius [float]: The jet radius to use when declustering\n",
        "            (default is 0.8)\n",
        "\n",
        "    Returns:\n",
        "        subjet_idx [np.array]: NumPy array of shape ``[num_particles]`` with elements\n",
        "                                representing which subjet the particle belongs to\n",
        "        subjet_vectors [list]: includes bjet information (e.g. px, py, pz)\n",
        "\n",
        "    \"\"\"\n",
        "    import awkward as ak\n",
        "    import fastjet\n",
        "    import vector\n",
        "\n",
        "    if JET_ALGO == \"kt\":\n",
        "        JET_ALGO = fastjet.kt_algorithm\n",
        "    elif JET_ALGO == \"antikt\":\n",
        "        JET_ALGO = fastjet.antikt_algorithm\n",
        "    elif JET_ALGO == \"CA\":\n",
        "        JET_ALGO = fastjet.cambridge_algorithm\n",
        "\n",
        "    jetdef = fastjet.JetDefinition(JET_ALGO, jet_radius)\n",
        "\n",
        "    # define jet directly not an array of jets\n",
        "    jet = ak.zip(\n",
        "        {\n",
        "            \"px\": px,\n",
        "            \"py\": py,\n",
        "            \"pz\": pz,\n",
        "            \"E\": e,\n",
        "        },\n",
        "        with_name=\"MomentumArray4D\",\n",
        "    )\n",
        "\n",
        "    pseudojet = [\n",
        "        fastjet.PseudoJet(particle.px.item(), particle.py.item(), particle.pz.item(), particle.E.item()) for particle in jet\n",
        "    ]\n",
        "\n",
        "    cluster = fastjet.ClusterSequence(pseudojet, jetdef)\n",
        "\n",
        "    # cluster jets\n",
        "    jets = cluster.inclusive_jets()\n",
        "    print(len(jets))\n",
        "    #assert len(jets) == 1\n",
        "\n",
        "    # get the 3 exclusive jets\n",
        "    subjets = cluster.exclusive_subjets(jets[0], N_SUBJETS)\n",
        "    assert len(subjets) == N_SUBJETS\n",
        "\n",
        "    # sort by pt\n",
        "    subjets = sorted(subjets, key=lambda x: x.pt(), reverse=True)\n",
        "\n",
        "    # define a subjet_idx placeholder\n",
        "    subjet_idx = ak.zeros_like(px, dtype=int) - 1\n",
        "    mapping = subjet_idx.to_list()\n",
        "\n",
        "    subjet_indices = []\n",
        "    for subjet_idx, subjet in enumerate(subjets):\n",
        "        subjet_indices.append([])\n",
        "        for subjet_const in subjet.constituents():\n",
        "            for idx, jet_const in enumerate(pseudojet):\n",
        "                if (\n",
        "                    subjet_const.px() == jet_const.px()\n",
        "                    and subjet_const.py() == jet_const.py()\n",
        "                    and subjet_const.pz() == jet_const.pz()\n",
        "                    and subjet_const.E() == jet_const.E()\n",
        "                ):\n",
        "                    subjet_indices[-1].append(idx)\n",
        "\n",
        "    for subjet_idx, subjet in enumerate(subjets):\n",
        "        local_mapping = np.array(mapping)\n",
        "        local_mapping[subjet_indices[subjet_idx]] = subjet_idx\n",
        "        mapping = local_mapping\n",
        "\n",
        "    # add the jet index\n",
        "    jet[\"subjet_idx\"] = ak.Array(mapping)\n",
        "\n",
        "    subjet_vectors = [\n",
        "        vector.obj(\n",
        "            px=ak.sum(jet.px[jet.subjet_idx == j], axis=-1),\n",
        "            py=ak.sum(jet.py[jet.subjet_idx == j], axis=-1),\n",
        "            pz=ak.sum(jet.pz[jet.subjet_idx == j], axis=-1),\n",
        "            E=ak.sum(jet.E[jet.subjet_idx == j], axis=-1),\n",
        "        )\n",
        "        for j in range(0, N_SUBJETS)\n",
        "    ]\n",
        "\n",
        "    return jet[\"subjet_idx\"].to_numpy(), subjet_vectors\n",
        "\n",
        "\n",
        "def scaling_up(outpath, epoch, N_values=30, N_SUBJETS=3, JET_ALGO=\"CA\", jet_radius=0.8):\n",
        "    \"\"\"\n",
        "    Computes the distribution of edges connecting different subjets for different values of N.\n",
        "\n",
        "    Args:\n",
        "        outpath [str]: Path to load the Rscores pkl files\n",
        "        epoch [int]: The epoch at which to load the model\n",
        "            (if -1: best trained model, if 0: untrained model, otherwise loads the corresponding epoch)\n",
        "        N_values [int]: The different values of N to scan, which will be taken as range(N_values)\n",
        "            (default is 15)\n",
        "        N_SUBJETS [int]: Number of subjets to decluster the jet into\n",
        "            (default is 3)\n",
        "        JET_ALGO [str]: The jet declustering algorithm to use. Choices are [\"CA\", \"kt\", \"antikt\"]\n",
        "            (default is \"CA\")\n",
        "        jet_radius [float]: The jet radius to use when declustering\n",
        "            (default is 0.8)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # same: list of counters, for each N, of edges connecting the same subjet\n",
        "    top_same, qcd_same = np.array([0] * N_values), np.array([0] * N_values)\n",
        "    # diff: list of counters, for each N, of edges connecting different subjets\n",
        "    top_diff, qcd_diff = np.array([0] * N_values), np.array([0] * N_values)\n",
        "\n",
        "    if epoch == -1:\n",
        "        PATH = f\"{outpath}/Rscores_best\"\n",
        "        save_as = \"best\"\n",
        "        legend_title = \"Trained model\"\n",
        "    elif epoch == 0:\n",
        "        PATH = f\"{outpath}/Rscores_untrained\"\n",
        "        save_as = \"untrained\"\n",
        "        legend_title = \"Untrained model\"\n",
        "    else:\n",
        "        PATH = f\"{outpath}/Rscores_epoch_{epoch}\"\n",
        "        save_as = f\"epoch_{epoch}\"\n",
        "        legend_title = f\"Model at epoch {epoch}\"\n",
        "\n",
        "    # load the jet information\n",
        "    with open(f\"{PATH}/batch_y.pkl\", \"rb\") as handle:\n",
        "        batch_y_list = pkl.load(handle)\n",
        "    with open(f\"{PATH}/batch_p4.pkl\", \"rb\") as handle:\n",
        "        batch_p4_list = pkl.load(handle)\n",
        "\n",
        "    # load the edgeRscores and edge_index of each EdgeConv block\n",
        "    with open(f\"{PATH}/R_edges.pkl\", \"rb\") as handle:\n",
        "        R_edges_list = pkl.load(handle)\n",
        "    with open(f\"{PATH}/edge_index.pkl\", \"rb\") as handle:\n",
        "        edge_index_list = pkl.load(handle)\n",
        "\n",
        "    Num_jets = len(batch_p4_list)\n",
        "    print(f\"Total # of jets is {Num_jets}\")\n",
        "    for i in range(Num_jets):\n",
        "        # define the jet information\n",
        "        jet_label = batch_y_list[i]\n",
        "\n",
        "        px = batch_p4_list[i][:, 0]\n",
        "        py = batch_p4_list[i][:, 1]\n",
        "        pz = batch_p4_list[i][:, 2]\n",
        "        e = batch_p4_list[i][:, 3]\n",
        "\n",
        "        # define the edgeRscores and the edge_index of the last EdgeConv block\n",
        "        edge_Rscores = R_edges_list[i][\"edge_conv_2\"]\n",
        "        edge_index = edge_index_list[i][\"edge_conv_2\"]\n",
        "\n",
        "        # get subjets\n",
        "        try:\n",
        "            print(f\"- Declustering jet # {i} using {JET_ALGO} algorithm\")\n",
        "            subjet_idx, _ = get_subjets(px, py, pz, e, N_SUBJETS, JET_ALGO, jet_radius)\n",
        "        except AssertionError:\n",
        "            print(f\"skipping jet # {i}\")\n",
        "            continue\n",
        "\n",
        "        for N in range(N_values):\n",
        "            # N=0 doesn't make sense here\n",
        "            for edge in torch.topk(edge_Rscores, N + 1).indices:\n",
        "                if jet_label == 1:\n",
        "                    if subjet_idx[edge_index[0][edge]] != subjet_idx[edge_index[1][edge]]:\n",
        "                        top_diff[N] += 1\n",
        "                    else:\n",
        "                        top_same[N] += 1\n",
        "                else:\n",
        "                    if subjet_idx[edge_index[0][edge]] != subjet_idx[edge_index[1][edge]]:\n",
        "                        qcd_diff[N] += 1\n",
        "                    else:\n",
        "                        qcd_same[N] += 1\n",
        "\n",
        "    top_fraction = top_diff / (top_same + top_diff)\n",
        "    qcd_fraction = qcd_diff / (qcd_same + qcd_diff)\n",
        "\n",
        "    outpath = f\"{outpath}/scaling_up\"\n",
        "    if not os.path.exists(outpath):\n",
        "        os.makedirs(outpath)\n",
        "\n",
        "    with open(f\"{outpath}/top_fraction_{save_as}.pkl\", \"wb\") as f:\n",
        "        pkl.dump(top_fraction, f)\n",
        "    with open(f\"{outpath}/qcd_fraction_{save_as}.pkl\", \"wb\") as f:\n",
        "        pkl.dump(qcd_fraction, f)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    ax.plot(range(len(top_fraction)), top_fraction, label=\"Top\")\n",
        "    ax.plot(range(len(qcd_fraction)), qcd_fraction, label=\"QCD\")\n",
        "    ax.legend(title=legend_title)\n",
        "    ax.set_xlabel(r\"$N_{edges}$\", fontsize=20)\n",
        "    ax.set_ylabel(r\"$N_{edges \\ between \\ subjets}$ / $N_{edges}$\", fontsize=20)\n",
        "    fig.tight_layout()\n",
        "    plt.savefig(f\"{outpath}/scaling_up_{save_as}.pdf\")\n",
        "    print(f\"saved the plot as {outpath}/scaling_up_{save_as}.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qddUVf7qojLy"
      },
      "outputs": [],
      "source": [
        "subjets = get_subjets(pf_vectors[jet][0][0:num],pf_vectors[jet][1][0:num], pf_vectors[jet][2][0:num], pf_vectors[jet][3][0:num], N_SUBJETS=2,  JET_ALGO=\"kt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u29i_MT57kF1"
      },
      "outputs": [],
      "source": [
        "labels[6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZ5WM9PhshIV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxTu2GwN7kF2"
      },
      "outputs": [],
      "source": [
        "\n",
        "number = jet\n",
        "num = 0\n",
        "for b in np.squeeze(pf_mask[number]):\n",
        "    if b == 0:\n",
        "        break\n",
        "    num = num + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOpLW6s07kF2"
      },
      "outputs": [],
      "source": [
        "deta = pf_features[jet][15][0:num]\n",
        "dphi = pf_features[jet][16][0:num]\n",
        "pt = pf_features[jet][0][0:num]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNrGZOEi7kF2"
      },
      "outputs": [],
      "source": [
        "deta = pf_features[jet][15][0:num]\n",
        "dphi = pf_features[jet][16][0:num]\n",
        "pt = pf_features[jet][0][0:num]\n",
        "detaCH = []\n",
        "detaNH = []\n",
        "detaMuon = []\n",
        "detaElectron = []\n",
        "detaPhoton = []\n",
        "dphiCH = []\n",
        "dphiNH = []\n",
        "dphiMuon = []\n",
        "dphiElectron = []\n",
        "dphiPhoton = []\n",
        "ptCH = []\n",
        "ptNH = []\n",
        "ptPhoton = []\n",
        "ptMuon = []\n",
        "ptElectron = []\n",
        "for particle in range(len(deta)):\n",
        "    if pf_features[jet][6][particle] == 1:\n",
        "        detaCH.append(pf_features[jet][15][particle])\n",
        "        dphiCH.append(pf_features[jet][16][particle])\n",
        "        ptCH.append(pf_features[jet][0][particle])\n",
        "    elif pf_features[jet][7][particle] == 1:\n",
        "        detaNH.append(pf_features[jet][15][particle])\n",
        "        dphiNH.append(pf_features[jet][16][particle])\n",
        "        ptNH.append(pf_features[jet][0][particle])\n",
        "\n",
        "    elif pf_features[jet][8][particle] == 1:\n",
        "        detaPhoton.append(pf_features[jet][15][particle])\n",
        "        dphiPhoton.append(pf_features[jet][16][particle])\n",
        "        ptPhoton.append(pf_features[jet][0][particle])\n",
        "    elif pf_features[jet][9][particle] == 1:\n",
        "        detaElectron.append(pf_features[jet][15][particle])\n",
        "        dphiElectron.append(pf_features[jet][16][particle])\n",
        "        ptElectron.append(pf_features[jet][0][particle])\n",
        "\n",
        "    elif pf_features[jet][10][particle] == 1:\n",
        "        detaMuon.append(pf_features[jet][15][particle])\n",
        "        dphiMuon.append(pf_features[jet][16][particle])\n",
        "\n",
        "        ptMuon.append(pf_features[jet][0][particle])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVuoskjT7kF3"
      },
      "outputs": [],
      "source": [
        "attention[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qg-HrDlw7kF3"
      },
      "outputs": [],
      "source": [
        "attention[x][jet][:,0:num,0:num].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI8CaJKBnGIs"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBetWUtq7kF3"
      },
      "outputs": [],
      "source": [
        "attention[x][jet][:][0:num,0:num].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhIKnUZwrpPm"
      },
      "outputs": [],
      "source": [
        "attentionOnly.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_V1siHmFgpW"
      },
      "outputs": [],
      "source": [
        "attentionOnly = np.transpose(attentionOnly,[1,0,2,3,4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mHCOTIXr1o4"
      },
      "outputs": [],
      "source": [
        "attentionOnly.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oaMm9Va2H3T"
      },
      "outputs": [],
      "source": [
        "import scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zK2efwnh1yj7"
      },
      "outputs": [],
      "source": [
        "correlation = []\n",
        "for indexed in range(8):\n",
        "  correlation.append(scipy.stats.pearsonr(attention[0][jet][:,0:num,0:num][indexed].flatten(), attentionOnly[0][jet][:,0:num,0:num][indexed].flatten()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0Zhfwd4tYTY"
      },
      "outputs": [],
      "source": [
        "correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lu8px4yurrG"
      },
      "outputs": [],
      "source": [
        "CMS = {\n",
        "    # \"font.sans-serif\": [\"TeX Gyre Heros\", \"Helvetica\", \"Arial\"],\n",
        "    \"font.family\": \"sans-serif\",\n",
        "    \"mathtext.fontset\": \"custom\",\n",
        "    \"mathtext.rm\": \"TeX Gyre Heros\",\n",
        "    \"mathtext.bf\": \"TeX Gyre Heros:bold\",\n",
        "    \"mathtext.sf\": \"TeX Gyre Heros\",\n",
        "    \"mathtext.it\": \"TeX Gyre Heros:italic\",\n",
        "    \"mathtext.tt\": \"TeX Gyre Heros\",\n",
        "    \"mathtext.cal\": \"TeX Gyre Heros\",\n",
        "    \"mathtext.default\": \"regular\",\n",
        "    \"figure.figsize\": (8.0, 8.0),\n",
        "    \"font.size\": 14,\n",
        "    #\"text.usetex\": True,\n",
        "    \"axes.labelsize\": \"medium\",\n",
        "    \"axes.unicode_minus\": False,\n",
        "    \"xtick.labelsize\": \"small\",\n",
        "    \"ytick.labelsize\": \"small\",\n",
        "    # Make legends smaller\n",
        "    \"legend.fontsize\": \"x-small\",  # Adjusted to a smaller size\n",
        "    \"legend.handlelength\": 1.5,\n",
        "    \"legend.borderpad\": 0.5,\n",
        "    \"xtick.direction\": \"in\",\n",
        "    \"xtick.major.size\": 12,\n",
        "    \"xtick.minor.size\": 6,\n",
        "    \"xtick.major.pad\": 6,\n",
        "    \"xtick.top\": True,\n",
        "    \"xtick.major.top\": True,\n",
        "    \"xtick.major.bottom\": True,\n",
        "    \"xtick.minor.top\": True,\n",
        "    \"xtick.minor.bottom\": True,\n",
        "    \"xtick.minor.visible\": True,\n",
        "    \"ytick.direction\": \"in\",\n",
        "    \"ytick.major.size\": 12,\n",
        "    \"ytick.minor.size\": 6.0,\n",
        "    \"ytick.right\": True,\n",
        "    \"ytick.major.left\": True,\n",
        "    \"ytick.major.right\": True,\n",
        "    \"ytick.minor.left\": True,\n",
        "    \"ytick.minor.right\": True,\n",
        "    \"ytick.minor.visible\": True,\n",
        "    \"grid.alpha\": 0.8,\n",
        "    \"grid.linestyle\": \":\",\n",
        "    \"axes.linewidth\": 2,\n",
        "    \"savefig.transparent\": False,\n",
        "}\n",
        "plt.style.use(CMS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNfZOW3RPU5s"
      },
      "outputs": [],
      "source": [
        "random_matrix = np.random.rand(2, 2, 8, 128,128)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-jtK4KWPJpE"
      },
      "outputs": [],
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "srandom_matrix = softmax(random_matrix, axis = 4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming attention is already a list or array\n",
        "# Flattening the attention array\n",
        "flattened_attention = np.stack(attention).flatten()\n",
        "\n",
        "# Define number of bins for the probability distribution\n",
        "num_bins = 20\n",
        "\n",
        "# Define the bin edges between 0 and 1, using 20 evenly spaced bins\n",
        "bin_edges = np.linspace(0, 1, num_bins + 1)\n",
        "\n",
        "# Function to process data in chunks and compute histogram\n",
        "def process_in_chunks(attention_iterator, chunk_size=100000, bin_edges=bin_edges):\n",
        "    hist_counts = np.zeros(len(bin_edges) - 1)  # Initialize histogram counts for bins\n",
        "\n",
        "    # Process each chunk of attention data\n",
        "    for chunk in attention_iterator:\n",
        "        # Flatten the chunk to ensure it's 1D and processable by np.histogram\n",
        "        chunk = np.array(chunk).flatten()\n",
        "\n",
        "        # Calculate histogram for this chunk\n",
        "        hist, _ = np.histogram(chunk, bins=bin_edges)\n",
        "\n",
        "        # Accumulate the counts\n",
        "        hist_counts += hist\n",
        "\n",
        "    total_data_points = hist_counts.sum()  # Total number of points processed\n",
        "    probabilities = hist_counts / total_data_points  # Normalize to get probabilities\n",
        "    return probabilities\n",
        "\n",
        "# Simulate loading a large dataset in chunks (e.g., from a file or other source)\n",
        "def attention_generator(attention, chunk_size):\n",
        "    \"\"\"Simulate chunked data loader for large dataset.\"\"\"\n",
        "    for i in range(0, len(attention), chunk_size):\n",
        "        yield attention[i:i + chunk_size]\n",
        "\n",
        "# Process the data in chunks (using a generator)\n",
        "probabilities = process_in_chunks(attention_generator(flattened_attention, chunk_size=100000))\n",
        "\n",
        "# Manually set the bin centers to have equal bar spacing\n",
        "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2  # Calculate the centers of each bin\n",
        "equal_width = bin_edges[1] - bin_edges[0]  # Set equal width for all bars based on bin spacing\n",
        "\n",
        "# Create figure and axes\n",
        "fig, ax = plt.subplots(figsize=(6, 6), dpi=300)\n",
        "\n",
        "# Plot bar graph with equal-width bars\n",
        "ax.bar(bin_centers, probabilities, width=equal_width, log=False)  # No log scale for clearer visualization\n",
        "\n",
        "# Set custom x-tick locations and labels (optional)\n",
        "#ax.set_xticks(bin_centers, 0.1)\n",
        "#ax.set_xticklabels([f'{edge:.2f}' for edge in bin_centers], fontsize=10, fontweight='bold')\n",
        "\n",
        "# Set x and y axis labels\n",
        "ax.set_xlabel('Attention Score', fontsize=14)\n",
        "ax.set_ylabel('Probability', fontsize=14)\n",
        "plt.yscale('log')\n",
        "\n",
        "# Add a title\n",
        "#ax.set_title('Attention Distribution', fontsize=16)\n",
        "plt.savefig('/content/drive/MyDrive/networks/Plots/attentionDist.pdf', bbox_inches=\"tight\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Qt_GVnzZANHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiY0RIDLyqnr"
      },
      "outputs": [],
      "source": [
        "jet = 19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRUCGkhe7TsC"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "from matplotlib.colorbar import ColorbarBase\n",
        "from matplotlib.cm import ScalarMappable\n",
        "\n",
        "# Define a function to plot the particles and attention lines\n",
        "def plot_attention_with_particles(attention_head, jet, deta_all, dphi_all, pt_all, subjets_all, layer_number, head_number, pf_features, output_filename):\n",
        "\n",
        "    # Normalize pt values for alpha transparency\n",
        "    print(\"Normalizing pt values...\")\n",
        "    norm_pt = mcolors.Normalize(vmin=pt_all.min(), vmax=pt_all.max())\n",
        "\n",
        "    # Identify the lowest Pt subjet\n",
        "    unique_subjets = np.unique(subjets_all)\n",
        "    print(f\"Unique subjets: {unique_subjets}\")\n",
        "    lowest_pt_subjet = min(np.unique(subjets_all), key=lambda s: np.sum(pt_all[subjets_all == s]))\n",
        "    print(f\"Lowest Pt subjet: {lowest_pt_subjet}\")\n",
        "\n",
        "    # Use a bright colormap\n",
        "    colormap = plt.get_cmap('spring')\n",
        "\n",
        "    # Normalize attention for line transparency/width\n",
        "    print(\"Normalizing attention values...\")\n",
        "    norm_attention = mcolors.Normalize(vmin=attention_head.min(), vmax=attention_head.max())\n",
        "\n",
        "\n",
        "    # Set figure size and DPI for faster rendering\n",
        "    print(\"Setting up figure...\")\n",
        "    fig, ax = plt.subplots(figsize=(8, 6), dpi=300)  # Increase DPI for better rendering\n",
        "\n",
        "    # Preprocess data to plot particles in batches based on their properties\n",
        "    print(\"Categorizing particles...\")\n",
        "    particle_groups = {\n",
        "        'charged_hadron': {'deta': [], 'dphi': [], 'pt': [], 'color': [], 'alpha': [], 'marker': '^'},\n",
        "        'neutral_hadron': {'deta': [], 'dphi': [], 'pt': [], 'color': [], 'alpha': [], 'marker': 'v'},\n",
        "        'photon': {'deta': [], 'dphi': [], 'pt': [], 'color': [], 'alpha': [], 'marker': 'o'},\n",
        "        'electron': {'deta': [], 'dphi': [], 'pt': [], 'color': [], 'alpha': [], 'marker': 'P'},\n",
        "        'muon': {'deta': [], 'dphi': [], 'pt': [], 'color': [], 'alpha': [], 'marker': 'X'},\n",
        "        'default': {'deta': [], 'dphi': [], 'pt': [], 'color': [], 'alpha': [], 'marker': 'o'}\n",
        "    }\n",
        "\n",
        "    for i in range(len(deta_all)):\n",
        "        # Alpha transparency based on pt with a minimum of 0.5 and scaling with normalized pt\n",
        "        alpha = max(0.5, norm_pt(pt_all[i]))\n",
        "#if subjets_all[i] == lowest_pt_subjet:\n",
        "        #else:\n",
        "        other_subjet_idx = np.where(unique_subjets == subjets_all[i])[0][0]\n",
        "        color = colormap(other_subjet_idx / len(unique_subjets) * 0.7)  # Bright colors\n",
        "\n",
        "        # Determine the marker based on the particle type in pf_features\n",
        "        if pf_features[jet][6][i] == 1:  # Charged Hadron\n",
        "            group = 'charged_hadron'\n",
        "        elif pf_features[jet][7][i] == 1:  # Neutral Hadron\n",
        "            group = 'neutral_hadron'\n",
        "        elif pf_features[jet][8][i] == 1:  # Photon\n",
        "            group = 'photon'\n",
        "        elif pf_features[jet][9][i] == 1:  # Electron\n",
        "            group = 'electron'\n",
        "        elif pf_features[jet][10][i] == 1:  # Muon\n",
        "            group = 'muon'\n",
        "        else:\n",
        "            group = 'default'\n",
        "\n",
        "        # Store the particle data in the appropriate group for batch plotting\n",
        "        particle_groups[group]['deta'].append(deta_all[i])\n",
        "        particle_groups[group]['dphi'].append(dphi_all[i])\n",
        "        particle_groups[group]['color'].append(color)\n",
        "        particle_groups[group]['alpha'].append(alpha)\n",
        "\n",
        "    print(\"Plotting particles...\")\n",
        "    # Batch plot particles for each group\n",
        "    for group, data in particle_groups.items():\n",
        "        if data['deta']:  # Only plot if there are particles in this group\n",
        "            # Apply alpha transparency as an array for each particle\n",
        "            ax.scatter(data['deta'], data['dphi'], color=data['color'], alpha=data['alpha'], s=250, zorder=3, marker=data['marker'],\n",
        "                       edgecolors='black', linewidths=1.5, antialiased=False)  # Disable anti-aliasing for speed\n",
        "\n",
        "    # Plot attention lines between particles with values above 0.9\n",
        "    print(\"Plotting attention lines...\")\n",
        "    for i in range(attention_head.shape[0]):\n",
        "        for j in range(attention_head.shape[1]):\n",
        "            if i != j:  # No self-loops\n",
        "                # Attention value between particles i and j\n",
        "                attn_value = attention_head[i, j]\n",
        "                linestyle = ''\n",
        "                lineValue=1\n",
        "                if attn_value > 0:  # Only plot lines for attention values >= 0.9\n",
        "                    alpha = norm_attention(attn_value)  # Transparency based on attention\n",
        "                    lineValue = 1\n",
        "                    # Solid lines within the same subjet, dashed between different subjets\n",
        "                    linestyle = ''\n",
        "                    if subjets_all[i] == subjets_all[j]:\n",
        "                        linestyle = 'solid'\n",
        "                        lineValue = 1\n",
        "                    else:\n",
        "                        linestyle = 'dotted'\n",
        "                        linestyle = (0, (2, 2))\n",
        "                        lineValue = 1.3\n",
        "\n",
        "                    # Plot lines\n",
        "                    ax.plot([deta_all[i], deta_all[j]], [dphi_all[i], dphi_all[j]], color='black',\n",
        "                            alpha=alpha, linewidth=lineValue * alpha, linestyle=linestyle, antialiased=False)  # Disable anti-aliasing for speed\n",
        "\n",
        "    print(\"Adding legends...\")\n",
        "\n",
        "    # Add the subjet legend without Pt values\n",
        "    legend1 = ax.legend(handles=[\n",
        "        plt.Line2D([0], [0], marker='o', color='w', label=f'Subjet {int(subjet)}', markerfacecolor=colormap(subjet / len(unique_subjets) * 0.7), markersize=10)\n",
        "        for subjet in unique_subjets\n",
        "    ], loc='upper right', title=\"Subjets\")\n",
        "\n",
        "    # Add the particle shape legend\n",
        "    #legend2 = ax.legend(handles=[\n",
        "    #    plt.Line2D([0], [0], marker=group['marker'], color='w', label=label, markerfacecolor='black', markersize=10)\n",
        "    #    for label, group in zip(['Charged Hadron', 'Neutral Hadron', 'Photon', 'Electron', 'Muon'], particle_groups.values())\n",
        "    #], loc='lower right', title=\"Particle Shapes\")\n",
        "\n",
        "    # Add all legends to the plot\n",
        "    ax.add_artist(legend1)\n",
        "    #ax.add_artist(legend2)\n",
        "\n",
        "    # Set axis labels without bold styling\n",
        "    ax.set_xlabel(r'$\\Delta \\eta$')\n",
        "    ax.set_ylabel(r'$\\Delta \\phi$')\n",
        "\n",
        "    # Add title with layer and head information\n",
        "    #ax.set_title(f'Layer {layer_number + 1} - Head {head_number + 1}')\n",
        "    ax.set_title('Untrained Model')\n",
        "\n",
        "    #ax.set_xlim(-0.4, 0.7)\n",
        "    #ax.set_xticks(np.arange(-0.4, 0.7, 0.2))\n",
        "\n",
        "    # Add a colorbar for the attention weights at the bottom\n",
        "    #cbar_ax = fig.add_axes([0.15, -0.03, 0.7, 0.03])  # Add a colorbar axis below the plot\n",
        "    cmap = plt.cm.gray_r  # Use a gray colormap to represent attention weights\n",
        "    vmin=attention_head.min()\n",
        "    vmax=attention_head.max()\n",
        "    norm = plt.Normalize(vmin=attention_head.min(), vmax=attention_head.max())  # Normalize based on attention values\n",
        "    #cb = ColorbarBase(cbar_ax, cmap=cmap, norm=norm)\n",
        "    cb = plt.colorbar(ScalarMappable(norm=norm, cmap=cmap))\n",
        "    cb.set_label('Attention Weight')\n",
        "\n",
        "    # Save the figure instead of showing it\n",
        "    print(f\"Saving figure to {output_filename}...\")\n",
        "    #plt.savefig(output_filename, bbox_inches='tight')\n",
        "    print(\"Figure saved.\")\n",
        "\n",
        "# Example usage based on your context (assuming pf_features, pf_mask, and attention are already defined)\n",
        "number = jet\n",
        "num = 0\n",
        "for b in np.squeeze(pf_mask[number]):\n",
        "    if b == 0:\n",
        "        break\n",
        "    num += 1\n",
        "\n",
        "# Extract the 4-momentum components for the valid particles\n",
        "px = pf_vectors[jet][0][0:num]\n",
        "py = pf_vectors[jet][1][0:num]\n",
        "pz = pf_vectors[jet][2][0:num]\n",
        "e = pf_vectors[jet][3][0:num]\n",
        "\n",
        "# Get the subjets using the get_subjets function\n",
        "subjets, subjet_vectors = get_subjets(px, py, pz, e, N_SUBJETS=3, JET_ALGO=\"kt\")\n",
        "\n",
        "# Initialize and combine particle data from all types\n",
        "deta_all = []\n",
        "dphi_all = []\n",
        "pt_all = []\n",
        "subjets_all = []\n",
        "\n",
        "# Append all particle types\n",
        "def append_particles(deta, dphi, pt, subjets, deta_all, dphi_all, pt_all, subjets_all):\n",
        "    deta_all.extend(deta)\n",
        "    dphi_all.extend(dphi)\n",
        "    pt_all.extend(pt)\n",
        "    subjets_all.extend(subjets)\n",
        "\n",
        "# Process the particles and combine them into one list\n",
        "append_particles(pf_features[jet][15][0:num], pf_features[jet][16][0:num], pf_features[jet][0][0:num], subjets,\n",
        "                 deta_all, dphi_all, pt_all, subjets_all)\n",
        "\n",
        "# Convert lists to numpy arrays for plotting\n",
        "deta_all = np.array(deta_all)\n",
        "dphi_all = np.array(dphi_all)\n",
        "pt_all = np.array(pt_all)\n",
        "subjets_all = np.array(subjets_all)\n",
        "\n",
        "# Example attention data, where `x` is the layer number\n",
        "layer_number = 7  # Choose the layer\n",
        "Decay = 'Tbqq'\n",
        "for head_number in range(8):\n",
        "  plot_attention_with_particles(attention[layer_number][jet][head_number, 0:num, 0:num], jet, deta_all, dphi_all, pt_all, subjets_all, layer_number, head_number, pf_features, '/content/drive/MyDrive/networks/Plots/Jet' + str(jet) + str(Decay) + '/' + str(Decay) + '-layer'+str(layer_number + 1) + '-head' + str(head_number + 1) + '.pdf')\n",
        "  #plot_attention_with_particles(srandom_matrix[0][1][4, 0:num, 0:num], jet, deta_all, dphi_all, pt_all, subjets_all, layer_number, head_number, pf_features, '/content/drive/MyDrive/networks/Plots/Jet' + str(jet) + str(Decay) + '/' + 'randomAttentionMatrix' + str(Decay) + '-layer'+str(layer_number + 1) + '-head' + str(head_number + 1) + '.pdf')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1m8Q8xWiLxQE"
      },
      "outputs": [],
      "source": [
        "attention[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UKCli9H3uLg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "# Define a function to plot the particles and attention lines\n",
        "def plot_attention_with_particles(attention_head, jet, deta_all, dphi_all, pt_all, subjets_all, layer_number, head_number, pf_features):\n",
        "\n",
        "    # Normalize pt values for alpha transparency\n",
        "    norm_pt = mcolors.Normalize(vmin=pt_all.min(), vmax=pt_all.max())\n",
        "    # Normalize attention for line transparency/width\n",
        "    norm_attention = mcolors.Normalize(vmin=attention_head.min(), vmax=attention_head.max())\n",
        "\n",
        "    # Identify the lowest Pt subjet\n",
        "    unique_subjets, inverse_indices = np.unique(subjets_all, return_inverse=True)\n",
        "    subjet_pt_sum = np.array([np.sum(pt_all[subjets_all == subjet]) for subjet in unique_subjets])\n",
        "    lowest_pt_subjet = unique_subjets[np.argmin(subjet_pt_sum)]\n",
        "    print(\"Unique subjets: \", unique_subjets)\n",
        "    print(\"Lowest Pt subjet: \", lowest_pt_subjet)\n",
        "\n",
        "    # Use a bright colormap\n",
        "    colormap = plt.get_cmap('spring')\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # Prepare colors and markers for all particles\n",
        "    colors = np.zeros((len(deta_all), 4))\n",
        "    markers = np.full(len(deta_all), 'o', dtype='<U1')\n",
        "\n",
        "    # Set colors and markers\n",
        "    for i in range(len(deta_all)):\n",
        "        # Determine color\n",
        "        if subjets_all[i] == lowest_pt_subjet:\n",
        "            colors[i] = (0.993248, 0.906157, 0.143936, 1.0)  # Yellow\n",
        "        else:\n",
        "            colors[i] = colormap(inverse_indices[i] / len(unique_subjets) * 0.7)\n",
        "\n",
        "        # Determine marker\n",
        "        if pf_features[jet][6][i] == 1:  # Charged Hadron\n",
        "            markers[i] = '^'\n",
        "        elif pf_features[jet][7][i] == 1:  # Neutral Hadron\n",
        "            markers[i] = 'v'\n",
        "        elif pf_features[jet][8][i] == 1:  # Photon\n",
        "            markers[i] = 'o'\n",
        "        elif pf_features[jet][9][i] == 1:  # Electron\n",
        "            markers[i] = 'P'\n",
        "        elif pf_features[jet][10][i] == 1:  # Muon\n",
        "            markers[i] = 'X'\n",
        "\n",
        "    # Plot all particles in one call\n",
        "    scatter = ax.scatter(deta_all, dphi_all, c=colors, alpha=0.8, s=300, edgecolors='black', linewidths=1.5, marker='o')\n",
        "\n",
        "    # Plot attention lines between particles\n",
        "    for i in range(attention_head.shape[0]):\n",
        "        for j in range(attention_head.shape[1]):\n",
        "            if i != j:  # No self-loops\n",
        "                attn_value = attention_head[i, j]\n",
        "                if attn_value > 0.1:  # Threshold to reduce number of lines plotted\n",
        "                    alpha = norm_attention(attn_value)\n",
        "                    linestyle = 'solid' if subjets_all[i] == subjets_all[j] else 'dotted'\n",
        "                    ax.plot([deta_all[i], deta_all[j]], [dphi_all[i], dphi_all[j]], color='black',\n",
        "                            alpha=alpha, linewidth=3 * alpha, linestyle=linestyle)\n",
        "\n",
        "    # Create legends for subjet colors and particle shapes\n",
        "    subjet_handles = [\n",
        "        plt.Line2D([0], [0], marker='o', color='w', label=f'Subjet {int(subjet)}',\n",
        "                   markerfacecolor=(0.993248, 0.906157, 0.143936, 1.0) if subjet == lowest_pt_subjet else colormap(idx / len(unique_subjets) * 0.7),\n",
        "                   markersize=10)\n",
        "        for idx, subjet in enumerate(unique_subjets)\n",
        "    ]\n",
        "\n",
        "    particle_shape_handles = [\n",
        "        plt.Line2D([0], [0], marker=mk, color='w', label=label, markerfacecolor='black', markersize=10)\n",
        "        for mk, label in zip(['^', 'v', 'o', 'P', 'X'], ['Charged Hadron', 'Neutral Hadron', 'Photon', 'Electron', 'Muon'])\n",
        "    ]\n",
        "\n",
        "    # Add legends\n",
        "    ax.legend(handles=subjet_handles, loc='upper right', title=\"Subjets\")\n",
        "    ax.legend(handles=particle_shape_handles, loc='lower right', title=\"Particle Shapes\")\n",
        "\n",
        "    ax.set_xlabel(r'$\\mathbf{\\Delta \\eta}$', fontsize=20, labelpad=20, weight='bold')\n",
        "    ax.set_ylabel(r'$\\mathbf{\\Delta \\phi}$', fontsize=20, labelpad=20, weight='bold')\n",
        "    ax.set_title(f'Layer {layer_number + 1} - Head {head_number + 1}', fontsize=16)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage based on your context (assuming pf_features, pf_mask, and attention are already defined)\n",
        "number = jet\n",
        "num = np.sum(np.squeeze(pf_mask[number]) > 0)  # Count valid particles\n",
        "\n",
        "# Extract the 4-momentum components for the valid particles\n",
        "px = pf_vectors[jet][0][:num]\n",
        "py = pf_vectors[jet][1][:num]\n",
        "pz = pf_vectors[jet][2][:num]\n",
        "e = pf_vectors[jet][3][:num]\n",
        "\n",
        "# Get the subjets\n",
        "subjets, subjet_vectors = get_subjets(px, py, pz, e, N_SUBJETS=2, JET_ALGO=\"kt\")\n",
        "\n",
        "# Initialize and combine particle data\n",
        "deta_all = np.array(pf_features[jet][15][:num])\n",
        "dphi_all = np.array(pf_features[jet][16][:num])\n",
        "pt_all = np.array(pf_features[jet][0][:num])\n",
        "subjets_all = np.array(subjets)\n",
        "\n",
        "# Example attention data\n",
        "layer_number = 7  # Choose the layer\n",
        "head_number = 2  # Choose the head\n",
        "plot_attention_with_particles(attention[layer_number][jet][head_number, :num, :num], jet, deta_all, dphi_all, pt_all, subjets_all, layer_number, head_number, pf_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmEJqW-_pOLe",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "# Define a function to plot the particles and attention lines\n",
        "def plot_attention_with_particles(attention_head, jet, deta_all, dphi_all, pt_all, subjets_all, layer_number, head_number, pf_features):\n",
        "\n",
        "    # Normalize pt values for alpha transparency\n",
        "    norm_pt = mcolors.Normalize(vmin=pt_all.min(), vmax=pt_all.max())\n",
        "\n",
        "    # Identify the lowest Pt subjet\n",
        "    unique_subjets = np.unique(subjets_all)\n",
        "    print(\"Unique subjets: \", unique_subjets)\n",
        "    subjet_pt_sum = {subjet: np.sum(pt_all[subjets_all == subjet]) for subjet in unique_subjets}\n",
        "    lowest_pt_subjet = min(subjet_pt_sum, key=subjet_pt_sum.get)  # Subjet with the lowest Pt\n",
        "    print(\"Lowest Pt subjet: \", lowest_pt_subjet)\n",
        "\n",
        "    # Use a bright colormap\n",
        "    colormap = plt.get_cmap('spring')\n",
        "\n",
        "    # Normalize attention for line transparency/width\n",
        "    norm_attention = mcolors.Normalize(vmin=attention_head.min(), vmax=attention_head.max())\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(5, 4))\n",
        "\n",
        "    # For subjet color legend\n",
        "    subjet_handles = []\n",
        "    for subjet in unique_subjets:\n",
        "        color = (0.993248, 0.906157, 0.143936, 1.0) if subjet == lowest_pt_subjet else colormap(subjet / len(unique_subjets) * 0.7)\n",
        "        subjet_handles.append(plt.Line2D([0], [0], marker='o', color='w', label=f'Subjet {int(subjet)}',\n",
        "                                         markerfacecolor=color, markersize=10))\n",
        "\n",
        "    # For particle shape legend\n",
        "    particle_shape_handles = [\n",
        "        plt.Line2D([0], [0], marker='^', color='w', label='Charged Hadron', markerfacecolor='black', markersize=10),\n",
        "        plt.Line2D([0], [0], marker='v', color='w', label='Neutral Hadron', markerfacecolor='black', markersize=10),\n",
        "        plt.Line2D([0], [0], marker='o', color='w', label='Photon', markerfacecolor='black', markersize=10),\n",
        "        plt.Line2D([0], [0], marker='P', color='w', label='Electron', markerfacecolor='black', markersize=10),\n",
        "        plt.Line2D([0], [0], marker='X', color='w', label='Muon', markerfacecolor='black', markersize=10)\n",
        "    ]\n",
        "\n",
        "    # Plot particles with appropriate shapes based on particle type in pf_features\n",
        "    for i in range(len(deta_all)):\n",
        "        alpha = max(0.5, norm_pt(pt_all[i]))  # Alpha transparency based on pt with a minimum of 0.5\n",
        "\n",
        "        # Determine color based on subjet, with yellow for the lowest Pt subjet\n",
        "        if subjets_all[i] == lowest_pt_subjet:\n",
        "            color = (0.993248, 0.906157, 0.143936, 1.0)  # Set to yellow for the lowest Pt subjet\n",
        "        else:\n",
        "            other_subjet_idx = np.where(unique_subjets == subjets_all[i])[0][0]\n",
        "            color = colormap(other_subjet_idx / len(unique_subjets) * 0.7)  # Bright colors\n",
        "\n",
        "        # Determine the marker based on the particle type in pf_features\n",
        "        if pf_features[jet][6][i] == 1:  # Charged Hadron\n",
        "            marker = '^'\n",
        "        elif pf_features[jet][7][i] == 1:  # Neutral Hadron\n",
        "            marker = 'v'\n",
        "        elif pf_features[jet][8][i] == 1:  # Photon\n",
        "            marker = 'o'\n",
        "        elif pf_features[jet][9][i] == 1:  # Electron\n",
        "            marker = 'P'\n",
        "        elif pf_features[jet][10][i] == 1:  # Muon\n",
        "            marker = 'X'\n",
        "        else:\n",
        "            marker = 'o'  # Default marker if none of the conditions are met\n",
        "\n",
        "        # Plot particles\n",
        "        ax.scatter(deta_all[i], dphi_all[i], color=[color], alpha=alpha, s=50, zorder=3, marker=marker,\n",
        "                   edgecolors='black', linewidths=1.5)\n",
        "\n",
        "    # Plot attention lines between particles\n",
        "    for i in range(attention_head.shape[0]):\n",
        "        for j in range(attention_head.shape[1]):\n",
        "            if i != j:  # No self-loops\n",
        "                # Attention value between particles i and j\n",
        "                attn_value = attention_head[i, j]\n",
        "                if attn_value > 0:\n",
        "                    alpha = norm_attention(attn_value)  # Transparency based on attention\n",
        "\n",
        "                    # Solid lines within the same subjet, dashed between different subjets\n",
        "                    if subjets_all[i] == subjets_all[j]:\n",
        "                        linestyle = 'solid'\n",
        "                    else:\n",
        "                        linestyle = 'dotted'\n",
        "\n",
        "                    # Plot lines\n",
        "                    ax.plot([deta_all[i], deta_all[j]], [dphi_all[i], dphi_all[j]], color='black',\n",
        "                            alpha=alpha, linewidth=1 * alpha, linestyle=linestyle)\n",
        "\n",
        "    # Add the subjet legend\n",
        "    legend1 = ax.legend(handles=subjet_handles, loc='upper right', title=\"Subjets\")\n",
        "\n",
        "    # Add the particle shape legend\n",
        "    legend2 = ax.legend(handles=particle_shape_handles, loc='lower right', title=\"Particle Shapes\")\n",
        "\n",
        "    # Add both legends\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    ax.set_xlabel(r'$\\mathbf{\\Delta \\eta}$', fontsize=20, labelpad=20, weight='bold')\n",
        "    ax.set_ylabel(r'$\\mathbf{\\Delta \\phi}$', fontsize=20, labelpad=20, weight='bold')\n",
        "\n",
        "    # Add title with layer and head information\n",
        "    ax.set_title(f'Layer {layer_number + 1} - Head {head_number + 1}', fontsize=16)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Example usage based on your context (assuming pf_features, pf_mask, and attention are already defined)\n",
        "number = jet\n",
        "num = 0\n",
        "for b in np.squeeze(pf_mask[number]):\n",
        "    if b == 0:\n",
        "        break\n",
        "    num += 1\n",
        "\n",
        "# Extract the 4-momentum components for the valid particles\n",
        "px = pf_vectors[jet][0][0:num]\n",
        "py = pf_vectors[jet][1][0:num]\n",
        "pz = pf_vectors[jet][2][0:num]\n",
        "e = pf_vectors[jet][3][0:num]\n",
        "\n",
        "# Get the subjets using the get_subjets function\n",
        "subjets, subjet_vectors = get_subjets(px, py, pz, e, N_SUBJETS=2, JET_ALGO=\"kt\")\n",
        "\n",
        "# Initialize and combine particle data from all types\n",
        "deta_all = []\n",
        "dphi_all = []\n",
        "pt_all = []\n",
        "subjets_all = []\n",
        "\n",
        "# Append all particle types\n",
        "def append_particles(deta, dphi, pt, subjets, deta_all, dphi_all, pt_all, subjets_all):\n",
        "    deta_all.extend(deta)\n",
        "    dphi_all.extend(dphi)\n",
        "    pt_all.extend(pt)\n",
        "    subjets_all.extend(subjets)\n",
        "\n",
        "# Process the particles and combine them into one list\n",
        "append_particles(pf_features[jet][15][0:num], pf_features[jet][16][0:num], pf_features[jet][0][0:num], subjets,\n",
        "                 deta_all, dphi_all, pt_all, subjets_all)\n",
        "\n",
        "# Convert lists to numpy arrays for plotting\n",
        "deta_all = np.array(deta_all)\n",
        "dphi_all = np.array(dphi_all)\n",
        "pt_all = np.array(pt_all)\n",
        "subjets_all = np.array(subjets_all)\n",
        "\n",
        "# Example attention data, where `x` is the layer number\n",
        "layer_number = 7  # Choose the layer\n",
        "head_number = 2 # Choose the head\n",
        "plot_attention_with_particles(attention[layer_number][jet][head_number, 0:num, 0:num], jet, deta_all, dphi_all, pt_all, subjets_all, layer_number, head_number, pf_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr0gK4TTucHR"
      },
      "outputs": [],
      "source": [
        "plot_attention(\n",
        "    detaCH, dphiCH, ptCH,\n",
        "    detaNH, dphiNH, ptNH,\n",
        "    detaPhoton, dphiPhoton, ptPhoton,\n",
        "    detaElectron, dphiElectron, ptElectron,\n",
        "    detaMuon, dphiMuon, ptMuon,\n",
        "    pf_features[jet][0][0:num],  # Assuming this corresponds to `pt`\n",
        "    deta, dphi,\n",
        "    attention[x][jet][:,0:num,0:num],  # attention matrix\n",
        "    8,  # layer number\n",
        "    subjets[0][0:num],  # subjets\n",
        "    plot_single_head=True,  # Plot only one head\n",
        "    head_to_plot=5,  # Plot the 7th head (index 6)\n",
        "    show_subjet=None  # Show all subjets\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmuM7g2Iy0yO"
      },
      "outputs": [],
      "source": [
        "plot_attention(detaCH, dphiCH, ptCH, detaNH, dphiNH, ptNH,\n",
        "                   detaPhoton, dphiPhoton, ptPhoton,\n",
        "                   detaElectron, dphiElectron, ptElectron,\n",
        "                   detaMuon, dphiMuon, ptMuon,\n",
        "                   pt, deta, dphi, attention_matrix, layer_number, subjets,\n",
        "                   plot_single_head=False, head_to_plot=0, show_subjet=None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7L5az9kGZ1rv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90ERZ34tXjOT"
      },
      "outputs": [],
      "source": [
        "for x in range(8):\n",
        "    plot_attention(detaCH,detaNH, detaPhoton,detaElectron,detaMuon, dphiCH,dphiNH,dphiPhoton,dphiElectron,dphiMuon,ptCH,ptNH,ptPhoton,ptElectron,ptMuon, pf_features[jet][0][0:num], deta, dphi,attention[x][jet][:,0:num,0:num],subjets[0][0:num])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDGL49JaPD-G"
      },
      "outputs": [],
      "source": [
        "for x in range(8):\n",
        "    plot_attention(detaCH,detaNH, detaPhoton,detaElectron,detaMuon, dphiCH,dphiNH,dphiPhoton,dphiElectron,dphiMuon,ptCH,ptNH,ptPhoton,ptElectron,ptMuon, pf_features[jet][0][0:num], deta, dphi,  F.softmax(torch.from_numpy(attentionOnly[x][jet][:,0:num,0:num]), dim=2) ,subjets[0][0:num])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JN3laf6rtUAb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "def plot_attention(detaCH, dphiCH, ptCH, detaNH, dphiNH, ptNH,\n",
        "                   detaPhoton, dphiPhoton, ptPhoton,\n",
        "                   detaElectron, dphiElectron, ptElectron,\n",
        "                   detaMuon, dphiMuon, ptMuon,\n",
        "                   pt, eta, phi, attention_matrix, layer_number, subjets,\n",
        "                   plot_single_head=False, head_to_plot=0, show_subjet=None):\n",
        "    # Check if attention_matrix is a PyTorch tensor and convert it\n",
        "    if isinstance(attention_matrix, torch.Tensor):\n",
        "        attention_matrix = attention_matrix.numpy()\n",
        "\n",
        "    num_heads, n, _ = attention_matrix.shape\n",
        "\n",
        "    # Determine how many heads to plot\n",
        "    if plot_single_head:\n",
        "        heads_to_plot = [head_to_plot]\n",
        "        fig, axes = plt.subplots(1, 1, figsize=(8, 8))  # Only one plot, single head\n",
        "        axes = [axes]  # Wrap single axis in a list to maintain consistency\n",
        "    else:\n",
        "        heads_to_plot = range(num_heads)\n",
        "        num_cols = 4\n",
        "        num_rows = int(np.ceil(len(heads_to_plot) / num_cols))\n",
        "        fig, axes = plt.subplots(num_rows, num_cols, figsize=(25, 6 * num_rows))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "    # Normalize the pt values for color mapping\n",
        "    norm_pt = mcolors.Normalize(vmin=pt.min(), vmax=pt.max())\n",
        "    cmap_pt = plt.cm.plasma\n",
        "\n",
        "    # Normalize the attention values for color mapping\n",
        "    norm_attn = mcolors.Normalize(vmin=attention_matrix.min(), vmax=attention_matrix.max())\n",
        "    cmap_attn = plt.cm.Greys\n",
        "\n",
        "    # Define a list of sequential colormaps for subjets\n",
        "    colormaps = [\n",
        "        plt.cm.Blues, plt.cm.Oranges, plt.cm.Greens,\n",
        "        plt.cm.Reds, plt.cm.Purples, plt.cm.Greys\n",
        "    ]\n",
        "\n",
        "    # Ensure we have enough colormaps for the number of subjets\n",
        "    num_subjets = np.max(subjets) + 1\n",
        "    assert num_subjets <= len(colormaps), \"Not enough colormaps defined for the number of subjets.\"\n",
        "\n",
        "    # Assign a distinct colormap to each subjet\n",
        "    subjet_colors = [colormaps[i % len(colormaps)] for i in range(num_subjets)]\n",
        "\n",
        "    # Define distinct colors for connection lines between subjets\n",
        "    from itertools import combinations\n",
        "    color_palette = list(mcolors.TABLEAU_COLORS.values())[:num_subjets * (num_subjets + 1) // 2]\n",
        "    connection_colors = {}\n",
        "    color_idx = 0\n",
        "\n",
        "    for i in range(num_subjets):\n",
        "        # Use the primary color of the subjet's colormap for self-connections\n",
        "        primary_color = subjet_colors[i](0.5)  # Use the midpoint of the colormap\n",
        "        connection_colors[(i, i)] = primary_color\n",
        "\n",
        "    for i, j in combinations(range(num_subjets), 2):\n",
        "        connection_colors[(i, j)] = color_palette[color_idx]\n",
        "        connection_colors[(j, i)] = color_palette[color_idx]\n",
        "        color_idx += 1\n",
        "\n",
        "    # Filter the attention matrix based on the show_subjet\n",
        "    valid_indices = range(n)\n",
        "    if show_subjet is not None:\n",
        "        valid_indices = [i for i in range(n) if subjets[i] == show_subjet]\n",
        "        attention_matrix_filtered = np.zeros_like(attention_matrix)\n",
        "        for head in range(num_heads):\n",
        "            for i in valid_indices:\n",
        "                for j in range(n):\n",
        "                    if subjets[j] == show_subjet or subjets[j] != -1:\n",
        "                        attention_matrix_filtered[head, i, j] = attention_matrix[head, i, j]\n",
        "                        attention_matrix_filtered[head, j, i] = attention_matrix[head, j, i]\n",
        "    else:\n",
        "        attention_matrix_filtered = attention_matrix\n",
        "\n",
        "    for head_idx, head in enumerate(heads_to_plot):\n",
        "        ax = axes[head_idx]\n",
        "        attn = attention_matrix_filtered[head]\n",
        "\n",
        "        # Gather and sort attention pairs by value\n",
        "        attention_pairs = [(attn[i, j], i, j) for i in range(n) for j in range(n) if i != j and attn[i, j] > 0]\n",
        "        attention_pairs.sort()  # Sort by attention value\n",
        "\n",
        "        # Plot attention lines from lowest to highest value (light to dark)\n",
        "        for value, i, j in attention_pairs:\n",
        "            alpha = 1 * norm_attn(value)\n",
        "\n",
        "            color = connection_colors.get((subjets[i], subjets[j]), 'black')\n",
        "            if subjets[i] == subjets[j]:\n",
        "                linestyle = 'solid'\n",
        "                linewidth = 1.5\n",
        "            elif show_subjet is not None and (subjets[i] == show_subjet or subjets[j] == show_subjet):\n",
        "                linestyle = 'dotted'\n",
        "                linewidth = 1.0\n",
        "            else:\n",
        "                linestyle = 'dashed'\n",
        "                linewidth = 2.5\n",
        "\n",
        "            ax.plot([eta[i], eta[j]], [phi[i], phi[j]],\n",
        "                    color=color, alpha=alpha, zorder=1, linewidth=linewidth, linestyle=linestyle)\n",
        "\n",
        "        # Function to get alpha, size, and edge width based on pt\n",
        "        def get_properties(index):\n",
        "            alpha = 0.5 + 0.5 * norm_pt(pt[index])  # Minimum opacity of 0.6\n",
        "            size = 50 + 150 * norm_pt(pt[index])  # Increase size for higher pt\n",
        "            edge_width = 0.5 + 2.0 * norm_pt(pt[index])  # Increase edge width for higher pt\n",
        "            return alpha, size, edge_width\n",
        "\n",
        "        # Function to plot particles with different shapes\n",
        "        def plot_particles(deta, dphi, pt_values, marker, label, subjet_indices):\n",
        "            for i in range(len(deta)):\n",
        "                alpha, size, edge_width = get_properties(i)\n",
        "                color = cmap_pt(norm_pt(pt_values[i]))  # Color by pt\n",
        "                edge_color = subjet_colors[subjet_indices[i]](0.5)  # Edge color by subjet\n",
        "                ax.scatter(deta[i], dphi[i], c=[color], alpha=alpha, s=size, zorder=3, marker=marker,\n",
        "                           edgecolors='black', linewidths=edge_width, label=label if i == 0 else \"\")\n",
        "\n",
        "        # Scatter plot with color by pt and outline color by subjet\n",
        "        plot_particles(detaCH, dphiCH, ptCH, '^', 'Charged Hadron', subjets)\n",
        "        plot_particles(detaNH, dphiNH, ptNH, 'v', 'Neutral Hadron', subjets)\n",
        "        plot_particles(detaPhoton, dphiPhoton, ptPhoton, 'o', 'Photon', subjets)\n",
        "        plot_particles(detaElectron, dphiElectron, ptElectron, 'P', 'Electron', subjets)\n",
        "        plot_particles(detaMuon, dphiMuon, ptMuon, 'X', 'Muon', subjets)\n",
        "\n",
        "        ax.set_title(f'Layer {layer_number}: Head {head + 1}')\n",
        "        ax.set_xlabel('eta')\n",
        "        ax.set_ylabel('phi')\n",
        "\n",
        "    # Remove extra subplots if plotting only one head\n",
        "    if not plot_single_head:\n",
        "        for i in range(len(heads_to_plot), num_rows * num_cols):\n",
        "            fig.delaxes(axes[i])\n",
        "\n",
        "    fig.tight_layout(pad=3.0, w_pad=3.0, h_pad=3.0)\n",
        "\n",
        "    # Add colorbars for pt and attention weights\n",
        "    cbar_pt = fig.colorbar(plt.cm.ScalarMappable(norm=norm_pt, cmap=cmap_pt), ax=axes[:len(heads_to_plot)],\n",
        "                           shrink=0.6, aspect=20, label='Pt')\n",
        "    cbar_attn = fig.colorbar(plt.cm.ScalarMappable(norm=norm_attn, cmap=cmap_attn), ax=axes[:len(heads_to_plot)],\n",
        "                             shrink=0.6, aspect=20, label='Attention Weight', orientation='horizontal', pad=0.05)\n",
        "\n",
        "    # Add legend for connection colors\n",
        "    unique_connections = {tuple(sorted((key[0], key[1]))): value for key, value in connection_colors.items()}\n",
        "    handles_conn = [plt.Line2D([0], [0], color=color, lw=2.5 if i != j else 1.5, linestyle='dashed' if i != j else 'solid') for (i, j), color in unique_connections.items()]\n",
        "    labels_conn = [f'Connection {i}-{j}' for (i, j) in unique_connections.keys()]\n",
        "    legend_conn = fig.legend(handles_conn, labels_conn, loc='upper right', title='Connection Colors', fontsize=12, title_fontsize=14)\n",
        "\n",
        "    # Add legend for particle shapes\n",
        "    handles_shapes = [plt.Line2D([0], [0], color='k', marker='^', linestyle='', markersize=10, label='Charged Hadron'),\n",
        "                      plt.Line2D([0], [0], color='k', marker='v', linestyle='', markersize=10, label='Neutral Hadron'),\n",
        "                      plt.Line2D([0], [0], color='k', marker='o', linestyle='', markersize=10, label='Photon'),\n",
        "                      plt.Line2D([0], [0], color='k', marker='P', linestyle='', markersize=10, label='Electron'),\n",
        "                      plt.Line2D([0], [0], color='k', marker='X', linestyle='', markersize=10, label='Muon')]\n",
        "    legend_shapes = fig.legend(handles_shapes, [handle.get_label() for handle in handles_shapes], loc='lower right', title='Particle Shapes', fontsize=12, title_fontsize=14)\n",
        "\n",
        "    # Add all legends to the figure\n",
        "    fig.add_artist(legend_conn)\n",
        "    fig.add_artist(legend_shapes)\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM3e-iRG2iwJ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXGahvAd7kF3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example tensor\n",
        "# attention[x][jet] could be a list of attention matrices, so we need to stack them\n",
        "attention_matrices = attention\n",
        "attention_array = np.stack(attention_matrices)\n",
        "print(attention_array.shape)\n",
        "# Compute the average across all layers\n",
        "# The axis=0 means we are averaging across the first dimension (layers)\n",
        "average_attention = np.mean(attention_array, axis=0)\n",
        "print(average_attention.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bnz1kQCoSJew"
      },
      "outputs": [],
      "source": [
        "average_attention[jet][:,0:num,0:num].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqUbsGd77kF3"
      },
      "outputs": [],
      "source": [
        "plot_pre_averaged_attention(detaCH, detaNH, detaPhoton, detaElectron, detaMuon,\n",
        "                       dphiCH, dphiNH, dphiPhoton, dphiElectron, dphiMuon,\n",
        "                       ptCH, ptNH, ptPhoton, ptElectron, ptMuon,\n",
        "                       pf_features[jet][0][0:num], deta, dphi, average_attention[jet][:,0:num,0:num], subjets[0], 'Head ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9Jug4JN-Dpq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "attention_matrices = attention\n",
        "attention_array = np.stack(attention_matrices)\n",
        "print(attention_array.shape)\n",
        "\n",
        "average_attention = np.mean(attention_array, axis=2)\n",
        "average_attention = np.transpose(average_attention, [1,0,2,3])\n",
        "\n",
        "print(average_attention.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiiTa9y8-WoI"
      },
      "outputs": [],
      "source": [
        "plot_pre_averaged_attention(detaCH, detaNH, detaPhoton, detaElectron, detaMuon,\n",
        "                       dphiCH, dphiNH, dphiPhoton, dphiElectron, dphiMuon,\n",
        "                       ptCH, ptNH, ptPhoton, ptElectron, ptMuon,\n",
        "                       pf_features[jet][0][0:num], deta, dphi, average_attention[jet][:,0:num,0:num], subjets[0],'Layer ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbTzz0r0Syne"
      },
      "outputs": [],
      "source": [
        "interaction[jet][:,0:num,0:num].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Qec9WmZAlWd"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "softmaxed_interaction = F.softmax((interaction[jet][:,0:num,0:num]), dim = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p3AuaGQt_Zb"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvqM8A4F7kF3"
      },
      "outputs": [],
      "source": [
        "plot_attention(detaCH,detaNH, detaPhoton,detaElectron,detaMuon, dphiCH,dphiNH,dphiPhoton,dphiElectron,dphiMuon,ptCH,ptNH,ptPhoton,ptElectron,ptMuon, pf_features[jet][0][0:num], deta, dphi,  softmaxed_interaction, 2, subjets[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgsYIKnnmxn9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def visualize_attention_pca(attention_matrix):\n",
        "    firstComponents = []\n",
        "\n",
        "    for layerNum in range(8):\n",
        "        attention_matrixes = torch.tensor(attention_matrix[layerNum][:, 0:num, 0:num])  # Replace with your actual attention matrix\n",
        "\n",
        "        # Set up the figure and axes for plotting\n",
        "        fig, axes = plt.subplots(2, 4, figsize=(16, 16))  # Make the figure more square\n",
        "        axes = axes.flatten()  # Flatten to easily iterate over\n",
        "\n",
        "        # Loop over each head (each 100x100 matrix)\n",
        "        for i in range(8):\n",
        "            head_matrix = attention_matrixes[i]  # Get the matrix for the i-th head\n",
        "\n",
        "            # Convert the PyTorch tensor to a NumPy array for PCA\n",
        "            head_matrix_np = head_matrix.numpy()\n",
        "\n",
        "            # Step 1: Fit PCA\n",
        "            pca = PCA(n_components=1)  # We use 1 component for the projection\n",
        "            pca.fit(head_matrix_np)\n",
        "\n",
        "            # Step 2: Get the first principal component\n",
        "            first_component = pca.components_[0]  # This is a vector of length num_particles\n",
        "            firstComponents.append(first_component)\n",
        "\n",
        "            # Step 3: Project the original data onto the first principal component\n",
        "            first_component_torch = torch.tensor(first_component)\n",
        "            projection_scores = head_matrix @ first_component_torch  # This gives you a (num_particles,) vector\n",
        "\n",
        "            # Step 4: Reconstruct the data using the first principal component\n",
        "            reconstructed_data = torch.ger(projection_scores, first_component_torch)\n",
        "\n",
        "            # Step 5: Apply softmax across rows or columns as needed\n",
        "            softmaxed_reconstructed_data = F.softmax(reconstructed_data, dim=1)\n",
        "\n",
        "            # Step 6: Plot the reconstructed data\n",
        "            ax = axes[i]\n",
        "            img = ax.imshow(softmaxed_reconstructed_data.numpy(), cmap='viridis', aspect='equal', origin='lower')  # Ensure square pixels\n",
        "            fig.colorbar(img, ax=ax)\n",
        "\n",
        "            ax.set_title(f'Layer {layerNum + 1} Head {i + 1}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return firstComponents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OL5-SwE1gnAY"
      },
      "outputs": [],
      "source": [
        "firstCompAttention[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87zXgboTpA1N"
      },
      "outputs": [],
      "source": [
        "atstack[jet][:,:,0:num,0:num].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UsjP7Id7kF4"
      },
      "outputs": [],
      "source": [
        "firstCompAttention = visualize_attention_pca(atstack[jet][:,:,0:num,0:num])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNkK2aN2f69a"
      },
      "outputs": [],
      "source": [
        "# Step 1: Fit PCA\n",
        "head_matrix = torch.tensor(atstack[jet][:,:,0:num,0:num][0][:,0:num,0:num])[0]  # Get the matrix for the i-th head\n",
        "\n",
        "# Convert the PyTorch tensor to a NumPy array for PCA\n",
        "head_matrix_np = head_matrix.numpy()\n",
        "pca = PCA(n_components=1)  # We use 1 component for the projection\n",
        "pca.fit(head_matrix_np)\n",
        "\n",
        "# Step 2: Get the first principal component\n",
        "projected_data = pca.fit_transform(head_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbZzVOQJ7kF4"
      },
      "outputs": [],
      "source": [
        "firstCompAttentionOnly = visualize_attention_pca(attentionOnly.transpose(1,0,2,3,4)[96][:,:,0:num,0:num])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIbSsqclvnDR"
      },
      "outputs": [],
      "source": [
        "firstCompAttention[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDGZ3RJEpZKh"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming firstCompAttention and firstCompAttentionOnly are lists or arrays of length 64\n",
        "# Each set of 8 entries corresponds to one layer\n",
        "\n",
        "for layer in range(8):  # 8 layers\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))  # Create a 2x4 grid for each layer\n",
        "    axes = axes.flatten()  # Flatten to easily iterate over\n",
        "\n",
        "    for i in range(8):  # Each layer has 8 plots\n",
        "        idx = layer * 8 + i  # Calculate the correct index for the current plot\n",
        "\n",
        "        # Create scatter plot for the current index\n",
        "        axes[i].scatter(firstCompAttention[idx], firstCompAttentionOnly[idx])\n",
        "        axes[i].set_xlabel('First Component Attention')\n",
        "        axes[i].set_ylabel('First Component Attention Only')\n",
        "        axes[i].set_title(f'Layer {layer + 1}, Head {i + 1}')\n",
        "\n",
        "    plt.tight_layout()  # Adjust spacing to prevent overlap\n",
        "    plt.show()  # Display the plot for the current layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRE6IxpJvUPC"
      },
      "outputs": [],
      "source": [
        "firstCompInteraction = visualize_attention_pca(intstack[96][:,:,0:num,0:num])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZU-MyroxDea"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Assuming firstCompAttention, firstCompAttentionOnly, and thirdComponent are lists or arrays of length 64\n",
        "# Each set of 8 entries corresponds to one layer\n",
        "\n",
        "for layer in range(8):  # 8 layers\n",
        "    fig = plt.figure(figsize=(20, 10))  # Create a new figure for each layer\n",
        "\n",
        "    # Create a 2x4 grid for each layer\n",
        "    axes = [fig.add_subplot(2, 4, i+1, projection='3d') for i in range(8)]\n",
        "\n",
        "    for i in range(8):  # Each layer has 8 plots\n",
        "        idx = layer * 8 + i  # Calculate the correct index for the current plot\n",
        "\n",
        "        # Create 3D scatter plot for the current index\n",
        "        axes[i].scatter(firstCompAttention[idx], firstCompAttentionOnly[idx], firstCompInteraction[idx])\n",
        "        axes[i].set_xlabel('First Component Attention')\n",
        "        axes[i].set_ylabel('First Component Attention Only')\n",
        "        axes[i].set_zlabel('First Component Interaction')\n",
        "        axes[i].set_title(f'Layer {layer + 1}, Head {i + 1}')\n",
        "\n",
        "    plt.tight_layout()  # Adjust spacing to prevent overlap\n",
        "    plt.show()  # Display the plot for the current layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dL2aPOICeERN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sLTQEwzJOvs"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming firstCompAttention and firstCompAttentionOnly are lists or arrays of length 64\n",
        "# Each set of 8 entries corresponds to one layer\n",
        "\n",
        "for layer in range(8):  # 8 layers\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))  # Create a 2x4 grid for each layer\n",
        "    axes = axes.flatten()  # Flatten to easily iterate over\n",
        "\n",
        "    for i in range(8):  # Each layer has 8 plots\n",
        "        idx = layer * 8 + i  # Calculate the correct index for the current plot\n",
        "\n",
        "        # Create scatter plot for the current index\n",
        "        axes[i].scatter(firstCompAttention[idx], firstCompAttentionOnly[idx])\n",
        "        axes[i].set_xlabel('First Component Attention')\n",
        "        axes[i].set_ylabel('First Component Interaction')\n",
        "        axes[i].set_title(f'Layer {layer + 1}, Head {i + 1}')\n",
        "\n",
        "    plt.tight_layout()  # Adjust spacing to prevent overlap\n",
        "    plt.show()  # Display the plot for the current layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gjxnswz9xrA7"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming firstCompAttention and firstCompAttentionOnly are lists or arrays of length 64\n",
        "# Each set of 8 entries corresponds to one layer\n",
        "\n",
        "for layer in range(8):  # 8 layers\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))  # Create a 2x4 grid for each layer\n",
        "    axes = axes.flatten()  # Flatten to easily iterate over\n",
        "\n",
        "    for i in range(8):  # Each layer has 8 plots\n",
        "        idx = layer * 8 + i  # Calculate the correct index for the current plot\n",
        "\n",
        "        # Create scatter plot for the current index\n",
        "        axes[i].scatter(np.log(firstCompAttention[idx]), firstCompInteraction[idx])\n",
        "        axes[i].set_xlabel('First Component Attention')\n",
        "        axes[i].set_ylabel('First Component Interaction')\n",
        "        axes[i].set_title(f'Layer {layer + 1}, Head {i + 1}')\n",
        "\n",
        "    plt.tight_layout()  # Adjust spacing to prevent overlap\n",
        "    plt.show()  # Display the plot for the current layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1un250U8yCLA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming firstCompAttention and firstCompAttentionOnly are lists or arrays of length 64\n",
        "# Each set of 8 entries corresponds to one layer\n",
        "\n",
        "for layer in range(8):  # 8 layers\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))  # Create a 2x4 grid for each layer\n",
        "    axes = axes.flatten()  # Flatten to easily iterate over\n",
        "\n",
        "    for i in range(8):  # Each layer has 8 plots\n",
        "        idx = layer * 8 + i  # Calculate the correct index for the current plot\n",
        "\n",
        "        # Create scatter plot for the current index\n",
        "        axes[i].scatter(firstCompAttentionOnly[idx], firstCompInteraction[idx])\n",
        "        axes[i].set_xlabel('First Component Attention Only')\n",
        "        axes[i].set_ylabel('First Component Interaction')\n",
        "        axes[i].set_title(f'Layer {layer + 1}, Head {i + 1}')\n",
        "\n",
        "    plt.tight_layout()  # Adjust spacing to prevent overlap\n",
        "    plt.show()  # Display the plot for the current layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1b7MnAvy16j"
      },
      "outputs": [],
      "source": [
        "np.stack(firstCompAttention).flatten().shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMDtpf4oz7-d"
      },
      "outputs": [],
      "source": [
        "pf_features[jet][x,0:num].flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3e31wd2y1LV"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming firstCompAttention and firstCompAttentionOnly are lists or arrays of length 64\n",
        "# Each set of 8 entries corresponds to one layer\n",
        "\n",
        "for features in range(17):\n",
        "  for layer in range(8):  # 8 layers\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))  # Create a 2x4 grid for each layer\n",
        "    axes = axes.flatten()  # Flatten to easily iterate over\n",
        "\n",
        "    for i in range(8):  # Each layer has 8 plots\n",
        "        idx = layer * 8 + i  # Calculate the correct index for the current plot\n",
        "\n",
        "        # Create scatter plot for the current index\n",
        "        axes[i].scatter(firstCompAttentionOnly[idx], pf_features[96][features,0:num])\n",
        "        axes[i].set_xlabel('First Component Attention Only')\n",
        "        axes[i].set_ylabel('Feature ' + str(features))\n",
        "        axes[i].set_title(f'Layer {layer + 1}, Head {i + 1}')\n",
        "\n",
        "    plt.tight_layout()  # Adjust spacing to prevent overlap\n",
        "    plt.show()  # Display the plot for the current layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HToSIgqM0Yi7"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming firstCompAttention and firstCompAttentionOnly are lists or arrays of length 64\n",
        "# Each set of 8 entries corresponds to one layer\n",
        "\n",
        "for features in range(17):\n",
        "  for layer in range(8):  # 8 layers\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))  # Create a 2x4 grid for each layer\n",
        "    axes = axes.flatten()  # Flatten to easily iterate over\n",
        "\n",
        "    for i in range(8):  # Each layer has 8 plots\n",
        "        idx = layer * 8 + i  # Calculate the correct index for the current plot\n",
        "\n",
        "        # Create scatter plot for the current index\n",
        "        axes[i].scatter(firstCompAttention[idx], pf_features[96][features,0:num])\n",
        "        axes[i].set_xlabel('First Component Attention')\n",
        "        axes[i].set_ylabel('Feature ' + str(features))\n",
        "        axes[i].set_title(f'Layer {layer + 1}, Head {i + 1}')\n",
        "\n",
        "    plt.tight_layout()  # Adjust spacing to prevent overlap\n",
        "    plt.show()  # Display the plot for the current layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vS4_wrLP0zXt"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming firstCompAttention and firstCompAttentionOnly are lists or arrays of length 64\n",
        "# Each set of 8 entries corresponds to one layer\n",
        "\n",
        "for features in range(17):\n",
        "  for layer in range(8):  # 8 layers\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))  # Create a 2x4 grid for each layer\n",
        "    axes = axes.flatten()  # Flatten to easily iterate over\n",
        "\n",
        "    for i in range(8):  # Each layer has 8 plots\n",
        "        idx = layer * 8 + i  # Calculate the correct index for the current plot\n",
        "\n",
        "        # Create scatter plot for the current index\n",
        "        axes[i].scatter(firstCompInteraction[idx], pf_features[96][features,0:num])\n",
        "        axes[i].set_xlabel('First Component Interaction')\n",
        "        axes[i].set_ylabel('Feature ' + str(features))\n",
        "        axes[i].set_title(f'Layer {layer + 1}, Head {i + 1}')\n",
        "\n",
        "    plt.tight_layout()  # Adjust spacing to prevent overlap\n",
        "    plt.show()  # Display the plot for the current layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwH-jXJ42HBq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert the list to a NumPy array if it's not already\n",
        "firstCompAttentionOnly = np.array(firstCompAttentionOnly)\n",
        "\n",
        "# Determine the size of the third dimension\n",
        "third_dim_size = firstCompAttentionOnly.size // (8 * 8)  # Calculate the size of the third dimension\n",
        "\n",
        "# Reshape to (layers, heads, third_dim_size)\n",
        "firstCompAttentionOnly_reshaped = firstCompAttentionOnly.reshape(8, 8, third_dim_size)\n",
        "\n",
        "# Average across all layers and heads for firstCompAttentionOnly\n",
        "average_firstCompAttentionOnly = np.mean(firstCompAttentionOnly_reshaped, axis=(0, 1))\n",
        "\n",
        "# Create a 5x4 grid for plotting\n",
        "fig, axes = plt.subplots(4, 5, figsize=(20, 15))  # 5 columns, 4 rows\n",
        "axes = axes.flatten()  # Flatten to easily iterate over\n",
        "\n",
        "for feature_idx in range(17):  # Iterate over each feature\n",
        "    # Scatter plot of the average data vs. the actual feature data\n",
        "    axes[feature_idx].scatter(average_firstCompAttentionOnly, pf_features[96][feature_idx, 0:num])\n",
        "    axes[feature_idx].set_xlabel('Avg First Component Attention Only')\n",
        "    axes[feature_idx].set_ylabel(f'Feature {feature_idx + 1}')\n",
        "    axes[feature_idx].set_title(f'Feature {feature_idx + 1}')\n",
        "\n",
        "# Adjust layout to prevent overlap and improve readability\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLHlOqc92HPt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert the list to a NumPy array if it's not already\n",
        "firstCompAttention = np.array(firstCompAttention)\n",
        "\n",
        "# Determine the size of the third dimension\n",
        "third_dim_size = firstCompAttention.size // (8 * 8)  # Calculate the size of the third dimension\n",
        "\n",
        "# Reshape to (layers, heads, third_dim_size)\n",
        "firstCompAttentionOnly_reshaped = firstCompAttention.reshape(8, 8, third_dim_size)\n",
        "\n",
        "# Average across all layers and heads for firstCompAttentionOnly\n",
        "average_firstCompAttentionOnly = np.mean(firstCompAttentionOnly_reshaped, axis=(0, 1))\n",
        "\n",
        "# Create a 5x4 grid for plotting\n",
        "fig, axes = plt.subplots(4, 5, figsize=(20, 15))  # 5 columns, 4 rows\n",
        "axes = axes.flatten()  # Flatten to easily iterate over\n",
        "\n",
        "for feature_idx in range(17):  # Iterate over each feature\n",
        "    # Scatter plot of the average data vs. the actual feature data\n",
        "    axes[feature_idx].scatter(average_firstCompAttentionOnly, pf_features[jet][feature_idx, 0:num])\n",
        "    axes[feature_idx].set_xlabel('Avg First Component Attention')\n",
        "    axes[feature_idx].set_ylabel(f'Feature {feature_idx + 1}')\n",
        "    axes[feature_idx].set_title(f'Feature {feature_idx + 1}')\n",
        "\n",
        "# Adjust layout to prevent overlap and improve readability\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwY_8Run299Q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert the list to a NumPy array if it's not already\n",
        "firstCompInteraction = np.array(firstCompInteraction)\n",
        "\n",
        "# Determine the size of the third dimension\n",
        "third_dim_size = firstCompInteraction.size // (8 * 8)  # Calculate the size of the third dimension\n",
        "\n",
        "# Reshape to (layers, heads, third_dim_size)\n",
        "firstCompAttentionOnly_reshaped = firstCompInteraction.reshape(8, 8, third_dim_size)\n",
        "\n",
        "# Average across all layers and heads for firstCompAttentionOnly\n",
        "average_firstCompAttentionOnly = np.mean(firstCompAttentionOnly_reshaped, axis=(0, 1))\n",
        "\n",
        "# Create a 5x4 grid for plotting\n",
        "fig, axes = plt.subplots(4, 5, figsize=(20, 15))  # 5 columns, 4 rows\n",
        "axes = axes.flatten()  # Flatten to easily iterate over\n",
        "\n",
        "for feature_idx in range(17):  # Iterate over each feature\n",
        "    # Scatter plot of the average data vs. the actual feature data\n",
        "    axes[feature_idx].scatter(average_firstCompAttentionOnly, pf_features[jet][feature_idx, 0:num])\n",
        "    axes[feature_idx].set_xlabel('Avg First Component Interaction')\n",
        "    axes[feature_idx].set_ylabel(f'Feature {feature_idx + 1}')\n",
        "    axes[feature_idx].set_title(f'Feature {feature_idx + 1}')\n",
        "\n",
        "# Adjust layout to prevent overlap and improve readability\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKYNwBaD4Xaf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert the list to a NumPy array if it's not already\n",
        "firstCompInteraction = np.array(firstCompInteraction)\n",
        "\n",
        "# Determine the size of the third dimension\n",
        "third_dim_size = firstCompInteraction.size // (8 * 8)  # Calculate the size of the third dimension\n",
        "\n",
        "# Reshape to (layers, heads, third_dim_size)\n",
        "firstCompAttentionOnly_reshaped = firstCompInteraction.reshape(8, 8, third_dim_size)\n",
        "\n",
        "# Iterate over each feature\n",
        "for feature_idx in range(17):\n",
        "    # Create a 2x4 grid for plotting (8 plots, one for each head)\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))  # 4 columns, 2 rows\n",
        "    axes = axes.flatten()  # Flatten to easily iterate over\n",
        "\n",
        "    for head in range(8):  # Iterate over each head\n",
        "        # Average across all layers for the current head\n",
        "        average_firstCompAttentionOnly = np.mean(firstCompAttentionOnly_reshaped[:, head, :], axis=0)\n",
        "\n",
        "        # Scatter plot of the average data vs. the actual feature data\n",
        "        axes[head].scatter(average_firstCompAttentionOnly, pf_features[96][feature_idx, :third_dim_size])\n",
        "        axes[head].set_xlabel('Avg First Component Interaction')\n",
        "        axes[head].set_ylabel(f'Feature {feature_idx + 1}')\n",
        "        axes[head].set_title(f'Feature {feature_idx + 1} - Head {head + 1}')\n",
        "\n",
        "    # Adjust layout to prevent overlap and improve readability\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpPImhQH4cEt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert the list to a NumPy array if it's not already\n",
        "firstCompAttention = np.array(firstCompAttention)\n",
        "\n",
        "# Determine the size of the third dimension\n",
        "third_dim_size = firstCompAttentionOnly_reshaped.size // (8 * 8)  # Calculate the size of the third dimension\n",
        "\n",
        "# Reshape to (layers, heads, third_dim_size)\n",
        "firstCompAttentionOnly_reshaped = firstCompAttention.reshape(8, 8, third_dim_size)\n",
        "\n",
        "# Iterate over each feature\n",
        "for feature_idx in range(17):\n",
        "    # Create a 2x4 grid for plotting (8 plots, one for each head)\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))  # 4 columns, 2 rows\n",
        "    axes = axes.flatten()  # Flatten to easily iterate over\n",
        "\n",
        "    for head in range(8):  # Iterate over each head\n",
        "        # Average across all layers for the current head\n",
        "        average_firstCompAttentionOnly = np.mean(firstCompAttentionOnly_reshaped[:, head, :], axis=0)\n",
        "\n",
        "        # Scatter plot of the average data vs. the actual feature data\n",
        "        axes[head].scatter(average_firstCompAttentionOnly, pf_features[96][feature_idx, :third_dim_size])\n",
        "        axes[head].set_xlabel('Avg First Component Interaction')\n",
        "        axes[head].set_ylabel(f'Feature {feature_idx + 1}')\n",
        "        axes[head].set_title(f'Feature {feature_idx + 1} - Head {head + 1}')\n",
        "\n",
        "    # Adjust layout to prevent overlap and improve readability\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XANxJ-2TUGF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert the list to a NumPy array if it's not already\n",
        "firstCompAttention = np.array(firstCompAttention)\n",
        "\n",
        "# Determine the size of the third dimension\n",
        "third_dim_size = firstCompAttentionOnly_reshaped.size // (8 * 8)  # Calculate the size of the third dimension\n",
        "\n",
        "# Reshape to (layers, heads, third_dim_size)\n",
        "firstCompAttentionOnly_reshaped = firstCompAttention.reshape(8, 8, third_dim_size)\n",
        "\n",
        "# Iterate over each feature\n",
        "for feature_idx in range(17):\n",
        "    # Create a 2x4 grid for plotting (8 plots, one for each layer)\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))  # 4 columns, 2 rows\n",
        "    axes = axes.flatten()  # Flatten to easily iterate over\n",
        "\n",
        "    for layer in range(8):  # Iterate over each layer\n",
        "        # Average across all heads for the current layer\n",
        "        average_firstCompAttentionOnly = np.mean(firstCompAttentionOnly_reshaped[layer], axis=0)\n",
        "\n",
        "        # Scatter plot of the average data vs. the actual feature data\n",
        "        axes[layer].scatter(average_firstCompAttentionOnly, pf_features[96][feature_idx, :third_dim_size])\n",
        "        axes[layer].set_xlabel('Avg First Component Interaction')\n",
        "        axes[layer].set_ylabel(f'Feature {feature_idx + 1}')\n",
        "        axes[layer].set_title(f'Feature {feature_idx + 1} - Layer {layer + 1}')\n",
        "\n",
        "    # Adjust layout to prevent overlap and improve readability\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jng4CroTGlb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert the list to a NumPy array if it's not already\n",
        "firstCompAttention = np.array(firstCompAttention)\n",
        "\n",
        "# Determine the size of the third dimension\n",
        "third_dim_size = firstCompAttentionOnly_reshaped.size // (8 * 8)  # Calculate the size of the third dimension\n",
        "\n",
        "# Reshape to (layers, heads, third_dim_size)\n",
        "firstCompAttentionOnly_reshaped = firstCompAttention.reshape(8, 8, third_dim_size)\n",
        "\n",
        "# Iterate over each feature\n",
        "for feature_idx in range(17):\n",
        "    # Create a 2x4 grid for plotting (8 plots, one for each head)\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))  # 4 columns, 2 rows\n",
        "    axes = axes.flatten()  # Flatten to easily iterate over\n",
        "\n",
        "    for head in range(8):  # Iterate over each head\n",
        "        # Average across all layers for the current head\n",
        "        average_firstCompAttentionOnly = np.mean(firstCompAttentionOnly_reshaped[:, head, :], axis=0)\n",
        "\n",
        "        # Scatter plot of the average data vs. the actual feature data\n",
        "        axes[head].scatter(average_firstCompAttentionOnly, pf_features[96][feature_idx, :third_dim_size])\n",
        "        axes[head].set_xlabel('Avg First Component Interaction')\n",
        "        axes[head].set_ylabel(f'Feature {feature_idx + 1}')\n",
        "        axes[head].set_title(f'Feature {feature_idx + 1} - Head {head + 1}')\n",
        "\n",
        "    # Adjust layout to prevent overlap and improve readability\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-X5s-THLPi32"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert the list to a NumPy array if it's not already\n",
        "firstCompAttentionOnly = np.array(firstCompAttentionOnly)\n",
        "\n",
        "# Determine the size of the third dimension\n",
        "third_dim_size = firstCompAttentionOnly_reshaped.size // (8 * 8)  # Calculate the size of the third dimension\n",
        "\n",
        "# Reshape to (layers, heads, third_dim_size)\n",
        "firstCompAttentionOnly_reshaped = firstCompAttentionOnly.reshape(8, 8, third_dim_size)\n",
        "\n",
        "# Iterate over each feature\n",
        "for feature_idx in range(17):\n",
        "    # Create a 2x4 grid for plotting (8 plots, one for each head)\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))  # 4 columns, 2 rows\n",
        "    axes = axes.flatten()  # Flatten to easily iterate over\n",
        "\n",
        "    for head in range(8):  # Iterate over each head\n",
        "        # Average across all layers for the current head\n",
        "        average_firstCompAttentionOnly = np.mean(firstCompAttentionOnly_reshaped[:, head, :], axis=0)\n",
        "\n",
        "        # Scatter plot of the average data vs. the actual feature data\n",
        "        axes[head].scatter(average_firstCompAttentionOnly, pf_features[96][feature_idx, :third_dim_size])\n",
        "        axes[head].set_xlabel('Avg First Component Interaction')\n",
        "        axes[head].set_ylabel(f'Feature {feature_idx + 1}')\n",
        "        axes[head].set_title(f'Feature {feature_idx + 1} - Head {head + 1}')\n",
        "\n",
        "    # Adjust layout to prevent overlap and improve readability\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert the list to a NumPy array if it's not already\n",
        "firstCompInteraction = np.array(firstCompInteraction)\n",
        "\n",
        "# Determine the size of the third dimension\n",
        "third_dim_size = firstCompInteraction.size // (8 * 8)  # Calculate the size of the third dimension\n",
        "\n",
        "# Reshape to (layers, heads, third_dim_size)\n",
        "firstCompAttentionOnly_reshaped = firstCompInteraction.reshape(8, 8, third_dim_size)\n",
        "\n",
        "# Iterate over each feature\n",
        "for feature_idx in range(17):\n",
        "    # Create a 2x4 grid for plotting (8 plots, one for each head)\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))  # 4 columns, 2 rows\n",
        "    axes = axes.flatten()  # Flatten to easily iterate over\n",
        "\n",
        "    for head in range(8):  # Iterate over each head\n",
        "        # Average across all layers for the current head\n",
        "        average_firstCompAttentionOnly = np.mean(firstCompAttentionOnly_reshaped[:, head, :], axis=0)\n",
        "\n",
        "        # Scatter plot of the average data vs. the actual feature data\n",
        "        axes[head].scatter(average_firstCompAttentionOnly, pf_features[96][feature_idx, :third_dim_size])\n",
        "        axes[head].set_xlabel('Avg First Component Interaction')\n",
        "        axes[head].set_ylabel(f'Feature {feature_idx + 1}')\n",
        "        axes[head].set_title(f'Feature {feature_idx + 1} - Head {head + 1}')\n",
        "\n",
        "    # Adjust layout to prevent overlap and improve readability\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPHRVAf-hqA3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}